{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_folder = r\"D:\\python\\data\\images\"   # Replace with your folder containing images\n",
    "output_json = r\"D:\\python\\data\\json\\labels.json\"     # Replace with your output folder\n",
    "output_boxed = r\"D:\\python\\data\\boxed\"     # Replace with your output folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Define the Character Set and Helper Functions\n",
    "# ------------------------------\n",
    "\n",
    "# In this example, we use a simple character set: lowercase letters and space.\n",
    "# For CTC loss, index 0 is reserved as the blank token.\n",
    "charset = ['අ', 'ආ', 'ඇ', 'ඈ', 'ඉ', 'ඊ', 'උ', 'ඌ', 'එ', 'ඒ', 'ඔ', 'ඕ', 'ඓ', 'ඖ',\n",
    "  'ක', 'කා', 'කැ', 'කෑ', 'කි', 'කී', 'කු', 'කූ', 'කෙ', 'කේ', 'කො', 'කෝ', 'කෛ', 'කෞ',\n",
    "  'ඛ', 'ඛා', 'ඛැ', 'ඛෑ', 'ඛි', 'ඛී', 'ඛු', 'ඛූ', 'ඛෙ', 'ඛේ', 'ඛො', 'ඛෝ', 'ඛෛ', 'ඛෞ',\n",
    "  'ග', 'ගා', 'ගැ', 'ගෑ', 'ගි', 'ගී', 'ගු', 'ගූ', 'ගෙ', 'ගේ', 'ගො', 'ගෝ', 'ගෛ', 'ගෞ',\n",
    "  'ච', 'චා', 'චැ', 'චෑ', 'චි', 'චී', 'චු', 'චූ', 'චෙ', 'චේ', 'චො', 'චෝ', 'චෛ', 'චෞ',\n",
    "  'ජ', 'ජා', 'ජැ', 'ජෑ', 'ජි', 'ජී', 'ජු', 'ජූ', 'ජෙ', 'ජේ', 'ජො', 'ජෝ', 'ජෛ', 'ජෞ',\n",
    "  'ට', 'ටා', 'ටැ', 'ටෑ', 'ටි', 'ටී', 'ටු', 'ටූ', 'ටෙ', 'ටේ', 'ටො', 'ටෝ', 'ටෛ', 'ටෞ',\n",
    "  'ඩ', 'ඩා', 'ඩැ', 'ඩෑ', 'ඩි', 'ඩී', 'ඩු', 'ඩූ', 'ඩෙ', 'ඩේ', 'ඩො', 'ඩෝ', 'ඩෛ', 'ඩෞ',\n",
    "  'ණ', 'ණා', 'ණැ', 'ණෑ', 'ණි', 'ණී', 'ණු', 'ණූ', 'ණෙ', 'ණේ', 'ණො', 'ණෝ', 'ණෛ', 'ණෞ',\n",
    "  'ත', 'තා', 'තැ', 'තෑ', 'ති', 'තී', 'තු', 'තූ', 'තෙ', 'තේ', 'තො', 'තෝ', 'තෛ', 'තෞ',\n",
    "  'ද', 'දා', 'දැ', 'දෑ', 'දි', 'දී', 'දු', 'දූ', 'දෙ', 'දේ', 'දො', 'දෝ', 'දෛ', 'දෞ',\n",
    "  'න', 'නා', 'නැ', 'නෑ', 'නි', 'නී', 'නු', 'නූ', 'නෙ', 'නේ', 'නො', 'නෝ', 'නෛ', 'නෞ',\n",
    "  'ප', 'පා', 'පැ', 'පෑ', 'පි', 'පී', 'පු', 'පූ', 'පෙ', 'පේ', 'පො', 'පෝ', 'පෛ', 'පෞ',\n",
    "  'බ', 'බා', 'බැ', 'බෑ', 'බි', 'බී', 'බු', 'බූ', 'බෙ', 'බේ', 'බො', 'බෝ', 'බෛ', 'බෞ',\n",
    "  'ම', 'මා', 'මැ', 'මෑ', 'මි', 'මී', 'මු', 'මූ', 'මෙ', 'මේ', 'මො', 'මෝ', 'මෛ', 'මෞ',\n",
    "  'ය', 'යා', 'යැ', 'යෑ', 'යි', 'යී', 'යු', 'යූ', 'යෙ', 'යේ', 'යො', 'යෝ', 'යෛ', 'යෞ',\n",
    "  'ර', 'රා', 'රැ', 'රෑ', 'රි', 'රී', 'රු', 'රූ', 'රෙ', 'රේ', 'රො', 'රෝ', 'රෛ', 'රෞ',\n",
    "  'ල', 'ලා', 'ලැ', 'ලෑ', 'ලි', 'ලී', 'ලු', 'ලූ', 'ලෙ', 'ලේ', 'ලො', 'ලෝ', 'ලෛ', 'ලෞ',\n",
    "  'ව', 'වා', 'වැ', 'වෑ', 'වි', 'වී', 'වු', 'වූ', 'වෙ', 'වේ', 'වො', 'වෝ', 'වෛ', 'වෞ',\n",
    "  'ශ', 'ශා', 'ශැ', 'ශෑ', 'ශි', 'ශී', 'ශු', 'ශූ', 'ශෙ', 'ශේ', 'ශො', 'ශෝ', 'ශෛ', 'ශෞ',\n",
    "  'ෂ', 'ෂා', 'ෂැ', 'ෂෑ', 'ෂි', 'ෂී', 'ෂු', 'ෂූ', 'ෂෙ', 'ෂේ', 'ෂො', 'ෂෝ', 'ෂෛ', 'ෂෞ',\n",
    "  'ස', 'සා', 'සැ', 'සෑ', 'සි', 'සී', 'සු', 'සූ', 'සෙ', 'සේ', 'සො', 'සෝ', 'සෛ', 'සෞ',\n",
    "  'හ', 'හා', 'හැ', 'හෑ', 'හි', 'හී', 'හු', 'හූ', 'හෙ', 'හේ', 'හො', 'හෝ', 'හෛ', 'හෞ',\n",
    "  'ළ', 'ළා', 'ළැ', 'ළෑ', 'ළි', 'ළී', 'ළු', 'ළූ', 'ළෙ', 'ළේ', 'ළො', 'ළෝ', 'ළෛ', 'ළෞ','ක්‍ර', 'ක්‍රි', 'ක්‍රී', 'ක්‍රා', 'ක්‍රැ', 'ක්‍රෑ',\n",
    "  'ප්‍ර', 'ප්‍රි', 'ප්‍රී', 'ප්‍රා',\n",
    "  'ද්‍ර', 'ද්‍රි', 'ද්‍රී', 'ද්‍රෝ',\n",
    "  'ත්‍ර', 'ත්‍රි', 'ත්‍රී', 'ත්‍රැ', 'ත්‍රෑ',\n",
    "  'ශ්‍ර', 'ශ්‍රි', 'ශ්‍රී', 'ශ්‍රැ', 'ශ්‍රෑ'\n",
    "            \n",
    "]\n",
    "char2idx = {c: i + 1 for i, c in enumerate(charset)}  # map characters to indices (starting at 1)\n",
    "num_classes = len(charset) + 1  # +1 for CTC blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text):\n",
    "    \"\"\"Convert a text string to a list of indices based on the charset.\"\"\"\n",
    "    return [char2idx[c] for c in text if c in char2idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2. Define a Custom Dataset for Multi-Word OCR\n",
    "# ------------------------------\n",
    "\n",
    "class OCRMultiWordDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, transform=None):\n",
    "        \"\"\"\n",
    "        images_dir: Directory of image files.\n",
    "        annotations_dir: Directory of JSON annotation files – one per image.\n",
    "            Each JSON file is expected to have the following structure:\n",
    "            [\n",
    "                {\n",
    "                    \"text\": \"word1\",\n",
    "                    \"bounding_box\": [x_min, y_min, x_max, y_max],\n",
    "                    \"confidence\": confidence_value\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        transform: Optional transform to be applied to a PIL image or NumPy array.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "        \n",
    "        # Load the single JSON file once\n",
    "        with open(annotations_dir, 'r', encoding='utf-8') as f:\n",
    "            raw = json.load(f)\n",
    "            # Convert list-of-dicts to a single dict\n",
    "            self.annotations = {}\n",
    "            for d in raw:\n",
    "                if isinstance(d, dict):\n",
    "                    self.annotations.update(d)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = self.image_files[idx]\n",
    "            img_path = os.path.join(self.images_dir, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image at {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = transforms.ToTensor()(image)\n",
    "\n",
    "           # Use image name (without extension) as key\n",
    "            img_key = os.path.splitext(img_name)[0]\n",
    "            \n",
    "            print(img_key)\n",
    "            if img_key.startswith(\"image_\"):\n",
    "                img_key = img_key.replace(\"image_\", \"\", 1)\n",
    "\n",
    "            if img_key not in self.annotations:\n",
    "                raise FileNotFoundError(f\"No annotation for image: {img_key}\")\n",
    "\n",
    "            ann_list = self.annotations[img_key]\n",
    "            print(f\"Processing {img_name} with {len(ann_list)} annotations\")\n",
    "            boxes = []\n",
    "            text_targets = []\n",
    "            target_lengths = []\n",
    "\n",
    "            _, H, W = image.shape\n",
    "            for ann in ann_list:\n",
    "                # Convert x, y, width, height to [x_min, y_min, x_max, y_max]\n",
    "                x_min = ann[\"x\"]\n",
    "                y_min = ann[\"y\"]\n",
    "                x_max = x_min + ann[\"width\"]\n",
    "                y_max = y_min + ann[\"height\"]\n",
    "                # Normalize\n",
    "                bbox_norm = [x_min / W, y_min / H, x_max / W, y_max / H]\n",
    "                boxes.append(bbox_norm)\n",
    "                text = ann[\"text\"].strip().lower()\n",
    "                seq = text_to_sequence(text)\n",
    "                text_targets.append(torch.tensor(seq, dtype=torch.long))\n",
    "                target_lengths.append(len(seq))\n",
    "\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            return image, boxes, text_targets, target_lengths\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {img_name}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 3. Create a Custom Collate Function\n",
    "# ------------------------------\n",
    "\n",
    "def ocr_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This collate function takes a list of samples from OCRMultiWordDataset.\n",
    "    Each sample: (image, boxes, text_targets, target_lengths)\n",
    "    It stacks the images into a batch tensor and keeps the others in lists.\n",
    "    \"\"\"\n",
    "    images = [item[0] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    boxes_list = [item[1] for item in batch]          # List of tensors (sizes vary per image)\n",
    "    text_targets_list = [item[2] for item in batch]     # List of lists (each inner list may have varying number)\n",
    "    target_lengths_list = [item[3] for item in batch]   # List of lists of ints\n",
    "\n",
    "    return images, boxes_list, text_targets_list, target_lengths_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 4. Define the Recognition Network\n",
    "# ------------------------------\n",
    "\n",
    "class TextRecognizer(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TextRecognizer, self).__init__()\n",
    "        # A simple CNN backbone.\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),   # (B, 64, H, W)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                           # (B, 64, H/2, W/2)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (B, 128, H/2, W/2)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)                            # (B, 128, H/4, W/4)\n",
    "        )\n",
    "        # After CNN, average pool over the height dimension to convert feature map into a sequence.\n",
    "        # In our example, we assume the crop has been resized to (32, 128) (height, width).\n",
    "        # After 2 pooling layers: height becomes 32/4 = 8, width becomes 128/4 = 32.\n",
    "        self.lstm = nn.LSTM(128, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(256 * 2, num_classes)  # bidirectional LSTM outputs hidden_dim*2 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (B, 3, 32, 128)\n",
    "        Returns:\n",
    "            Outputs of shape (T, B, num_classes), where T is the sequence length (here, 32).\n",
    "        \"\"\"\n",
    "        features = self.cnn(x)  # (B, 128, 8, 32)\n",
    "        # Average over the height dimension to form a sequence (time dimension = width)\n",
    "        features = torch.mean(features, dim=2)  # (B, 128, 32)\n",
    "        features = features.permute(0, 2, 1)      # (B, 32, 128)\n",
    "        outputs, _ = self.lstm(features)         # (B, 32, 512)\n",
    "        outputs = self.fc(outputs)               # (B, 32, num_classes)\n",
    "        outputs = outputs.permute(1, 0, 2)         # (T, B, num_classes) for CTC loss\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 5. Define the End‑to‑End OCR Model for Multi‑Word Images\n",
    "# ------------------------------\n",
    "\n",
    "class EndToEndOCRMulti(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EndToEndOCRMulti, self).__init__()\n",
    "        # For this demo, we focus on recognition.\n",
    "        # In a full system, a multi‑object detection branch would also be included.\n",
    "        self.recognizer = TextRecognizer(num_classes)\n",
    "\n",
    "    def forward(self, images, gt_boxes_list):\n",
    "        \"\"\"\n",
    "        images: Tensor of shape (B, 3, H, W)\n",
    "        gt_boxes_list: List of length B; each element is a tensor of shape (N, 4) where N is the number\n",
    "                       of text boxes in that image (normalized to [0,1]).\n",
    "        Returns: Recognition logits of shape (T, total_boxes, num_classes)\n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        batch_size = images.size(0)\n",
    "        for i in range(batch_size):\n",
    "            image = images[i:i+1]  # shape: (1, 3, H, W)\n",
    "            _, H, W = image.shape[1:]\n",
    "            # Loop over all bounding boxes for the current image.\n",
    "            for box in gt_boxes_list[i]:\n",
    "                # box = [x_min, y_min, x_max, y_max] in normalized coordinates.\n",
    "                x_min = int(box[0].item() * W)\n",
    "                y_min = int(box[1].item() * H)\n",
    "                x_max = int(box[2].item() * W)\n",
    "                y_max = int(box[3].item() * H)\n",
    "                x_min = max(x_min, 0)\n",
    "                y_min = max(y_min, 0)\n",
    "                x_max = min(x_max, W)\n",
    "                y_max = min(y_max, H)\n",
    "                # Crop the word region.\n",
    "                crop = image[:, :, y_min:y_max, x_min:x_max]\n",
    "                # Resize the crop to fixed size expected by the recognizer (e.g., 32x128).\n",
    "                crop = F.interpolate(crop, size=(32, 128), mode='bilinear', align_corners=False)\n",
    "                crops.append(crop)\n",
    "        if len(crops) == 0:\n",
    "            # In case no boxes are found, return an empty tensor.\n",
    "            return None\n",
    "        # Concatenate all cropped word regions into one batch.\n",
    "        crops = torch.cat(crops, dim=0)  # shape: (total_boxes, 3, 32, 128)\n",
    "        recog_logits = self.recognizer(crops)  # shape: (T, total_boxes, num_classes)\n",
    "        return recog_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "Epoch [1/10], Recognition Loss: -2.5312\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "Epoch [2/10], Recognition Loss: 8.3293\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "Epoch [3/10], Recognition Loss: 0.9625\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "Epoch [4/10], Recognition Loss: -0.4110\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "Epoch [5/10], Recognition Loss: 7.6544\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "Epoch [6/10], Recognition Loss: 6.9260\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "Epoch [7/10], Recognition Loss: 2.3075\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "Epoch [8/10], Recognition Loss: 4.1209\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "Epoch [9/10], Recognition Loss: 7.0023\n",
      "image_0a83e9db-7883-4397-9952-48fe7f047c4c\n",
      "Processing image_0a83e9db-7883-4397-9952-48fe7f047c4c.png with 22 annotations\n",
      "image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587\n",
      "Processing image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587.png with 76 annotations\n",
      "image_0a99c29c-0d80-4473-95bd-1a603bdf7830\n",
      "Processing image_0a99c29c-0d80-4473-95bd-1a603bdf7830.png with 93 annotations\n",
      "image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a\n",
      "Processing image_0a68cbad-c8b1-4aaa-b8b8-f49805f0978a.png with 55 annotations\n",
      "image_0a2b8b0f-358b-45f9-ad31-2421064ccce9\n",
      "Processing image_0a2b8b0f-358b-45f9-ad31-2421064ccce9.png with 19 annotations\n",
      "image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56\n",
      "Processing image_0a83bc5d-d8ca-4a37-bb5e-0eb9be952e56.png with 44 annotations\n",
      "image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5\n",
      "Processing image_0a68dd5a-ade7-4efe-94ca-4c49b62625a5.png with 42 annotations\n",
      "image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16\n",
      "Processing image_0a1caaa8-4fc5-4d1a-a9c9-04afe046ff16.png with 89 annotations\n",
      "Epoch [10/10], Recognition Loss: 5.1439\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define transforms – here just converting to tensor.\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "    # Update these paths to point to your dataset directories.\n",
    "    images_dir = input_folder\n",
    "    annotations_dir = output_json\n",
    "\n",
    "    dataset = OCRMultiWordDataset(images_dir, annotations_dir, transform=transform)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=ocr_collate_fn, num_workers=0)  # Set num_workers=0 for debugging\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EndToEndOCRMulti(num_classes).to(device)\n",
    "\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 10\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            if batch is None:\n",
    "                continue  # Skip problematic batches\n",
    "            images, boxes_list, text_targets_list, target_lengths_list = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            recog_logits = model(images, boxes_list)\n",
    "            if recog_logits is None or recog_logits.size(1) == 0:\n",
    "                print(\"Skipping batch due to empty recog_logits\")\n",
    "                continue\n",
    "\n",
    "            T, total_words, _ = recog_logits.size()\n",
    "            input_lengths = torch.full(size=(total_words,), fill_value=T, dtype=torch.long).to(device)\n",
    "\n",
    "            targets = []\n",
    "            all_target_lengths = []\n",
    "            for text_list, length_list in zip(text_targets_list, target_lengths_list):\n",
    "                for seq, length in zip(text_list, length_list):\n",
    "                    targets.append(seq)\n",
    "                    all_target_lengths.append(length)\n",
    "\n",
    "            if len(targets) == 0:\n",
    "                print(\"Skipping batch due to empty targets\")\n",
    "                continue\n",
    "\n",
    "            targets_cat = torch.cat(targets).to(device)\n",
    "            target_lengths = torch.tensor(all_target_lengths, dtype=torch.long).to(device)\n",
    "\n",
    "            if targets_cat.size(0) != target_lengths.sum():\n",
    "                print(\"Mismatch between targets_cat and target_lengths. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                loss_recog = ctc_loss_fn(recog_logits, targets_cat, input_lengths, target_lengths)\n",
    "                loss_recog.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss_recog.item()\n",
    "            except Exception as e:\n",
    "                print(f\"Error during loss computation: {e}\")\n",
    "                continue\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Recognition Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_text(seq):\n",
    "    # seq: list or numpy array of indices\n",
    "    # skips blanks (0) and out-of-range indices\n",
    "    return ''.join([charset[idx] for idx in seq if idx != 0 and idx < len(charset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00cd9460-a0de-4c24-b8b4-66115a729e7d.json\n",
      "Annotations: [{'text': 'මේ', 'bounding_box': [10, 12, 35, 32], 'confidence': 92}, {'text': 'අන්ඩුව', 'bounding_box': [50, 11, 106, 36], 'confidence': 91}, {'text': 'පත්\\u200c', 'bounding_box': [122, 11, 151, 32], 'confidence': 96}, {'text': 'කිරීම', 'bounding_box': [167, 12, 210, 32], 'confidence': 96}, {'text': 'ගෑන', 'bounding_box': [226, 18, 263, 35], 'confidence': 96}, {'text': 'මම', 'bounding_box': [279, 14, 307, 32], 'confidence': 95}, {'text': 'නම්\\u200c', 'bounding_box': [322, 12, 352, 32], 'confidence': 96}, {'text': 'දුක්\\u200c', 'bounding_box': [368, 11, 395, 36], 'confidence': 96}, {'text': 'වෙන්නේ', 'bounding_box': [411, 11, 479, 32], 'confidence': 96}, {'text': 'නෑ.', 'bounding_box': [10, 54, 31, 71], 'confidence': 96}, {'text': 'ඒකට', 'bounding_box': [51, 47, 95, 68], 'confidence': 96}, {'text': 'හේතුව', 'bounding_box': [110, 47, 166, 72], 'confidence': 94}, {'text': 'මෙන්න', 'bounding_box': [182, 47, 239, 68], 'confidence': 96}, {'text': 'මේකයි.', 'bounding_box': [255, 48, 310, 68], 'confidence': 96}, {'text': 'කාලෙකට', 'bounding_box': [330, 50, 407, 71], 'confidence': 96}, {'text': 'ඉස්සර', 'bounding_box': [423, 47, 476, 71], 'confidence': 96}, {'text': '80', 'bounding_box': [11, 88, 29, 104], 'confidence': 96}, {'text': 'දශකයේ', 'bounding_box': [45, 83, 112, 108], 'confidence': 96}, {'text': 'අග', 'bounding_box': [127, 90, 154, 107], 'confidence': 93}, {'text': 'භාගයෙදී', 'bounding_box': [170, 85, 242, 108], 'confidence': 92}, {'text': 'වාගේ', 'bounding_box': [257, 83, 303, 104], 'confidence': 93}, {'text': ';', 'bounding_box': [319, 100, 321, 106], 'confidence': 91}, {'text': 'ඊයෙ', 'bounding_box': [338, 87, 374, 104], 'confidence': 93}, {'text': 'පෙරෙදා', 'bounding_box': [389, 87, 450, 108], 'confidence': 92}, {'text': 'මියගිය', 'bounding_box': [11, 120, 67, 140], 'confidence': 96}, {'text': 'අජන්තා', 'bounding_box': [83, 119, 145, 143], 'confidence': 96}, {'text': 'රණසිංහ', 'bounding_box': [161, 121, 232, 140], 'confidence': 96}, {'text': 'ලොක්ක', 'bounding_box': [248, 119, 313, 143], 'confidence': 95}, {'text': 'තමයි', 'bounding_box': [328, 121, 372, 140], 'confidence': 93}, {'text': 'නවයුගය', 'bounding_box': [387, 122, 460, 144], 'confidence': 92}, {'text': 'පත්තරය', 'bounding_box': [10, 155, 81, 176], 'confidence': 93}, {'text': 'කලේ.', 'bounding_box': [97, 155, 140, 179], 'confidence': 88}, {'text': 'ඔය', 'bounding_box': [159, 158, 186, 176], 'confidence': 96}, {'text': 'කාලයේදීම', 'bounding_box': [202, 155, 286, 180], 'confidence': 95}, {'text': 'වගේ', 'bounding_box': [302, 155, 342, 176], 'confidence': 96}, {'text': 'නන්දා', 'bounding_box': [357, 155, 404, 180], 'confidence': 93}, {'text': 'මාලණී', 'bounding_box': [421, 156, 475, 179], 'confidence': 92}, {'text': '\"හේමන්ත්යෙදී\\'', 'bounding_box': [11, 191, 129, 216], 'confidence': 84}, {'text': 'අවසාන', 'bounding_box': [145, 194, 206, 215], 'confidence': 96}, {'text': 'කරලා', 'bounding_box': [221, 195, 270, 215], 'confidence': 91}, {'text': \"'සත්යයේ\", 'bounding_box': [286, 191, 360, 212], 'confidence': 40}, {'text': 'ගීතය\"', 'bounding_box': [375, 193, 426, 212], 'confidence': 89}, {'text': 'පටන්\\u200c', 'bounding_box': [442, 191, 486, 212], 'confidence': 96}, {'text': 'ගත්තේ.ඒ', 'bounding_box': [10, 227, 87, 248], 'confidence': 71}, {'text': 'කාලයේ', 'bounding_box': [102, 227, 163, 251], 'confidence': 96}, {'text': 'මේ', 'bounding_box': [178, 228, 203, 248], 'confidence': 93}, {'text': 'ප්රසන්ග', 'bounding_box': [218, 227, 289, 248], 'confidence': 89}, {'text': 'වල', 'bounding_box': [305, 230, 333, 251], 'confidence': 93}, {'text': 'ගැයුනු', 'bounding_box': [348, 234, 399, 252], 'confidence': 90}, {'text': 'ගීත', 'bounding_box': [414, 229, 446, 248], 'confidence': 96}, {'text': 'වලට', 'bounding_box': [461, 230, 502, 251], 'confidence': 95}, {'text': 'අණ්ඩුවේ', 'bounding_box': [10, 264, 81, 288], 'confidence': 82}, {'text': 'පත්තර', 'bounding_box': [97, 263, 155, 284], 'confidence': 96}, {'text': 'වලින්\\u200c', 'bounding_box': [170, 263, 215, 287], 'confidence': 96}, {'text': 'ඉස්ම', 'bounding_box': [230, 263, 271, 287], 'confidence': 93}, {'text': 'යන්න', 'bounding_box': [287, 263, 333, 284], 'confidence': 96}, {'text': 'ගැහුවා.', 'bounding_box': [348, 266, 404, 288], 'confidence': 96}, {'text': 'ඉතින්\\u200c', 'bounding_box': [424, 263, 470, 287], 'confidence': 96}, {'text': 'මටත්\\u200c', 'bounding_box': [11, 299, 54, 320], 'confidence': 96}, {'text': 'සෙට්\\u200c', 'bounding_box': [70, 300, 107, 320], 'confidence': 95}, {'text': 'උනා', 'bounding_box': [123, 306, 155, 323], 'confidence': 96}, {'text': 'එක', 'bounding_box': [171, 302, 202, 320], 'confidence': 97}, {'text': 'එක', 'bounding_box': [217, 302, 248, 320], 'confidence': 96}, {'text': 'කලා', 'bounding_box': [263, 306, 300, 323], 'confidence': 92}, {'text': 'බුවාලගෙන්\\u200c', 'bounding_box': [315, 299, 407, 324], 'confidence': 92}, {'text': 'ටෝක්ස්\\u200c', 'bounding_box': [422, 299, 483, 320], 'confidence': 95}, {'text': 'ටිකක්\\u200c', 'bounding_box': [10, 335, 58, 356], 'confidence': 93}, {'text': 'ගන්නදවසක්\\u200c', 'bounding_box': [74, 335, 181, 360], 'confidence': 89}, {'text': 'මටසෙට්\\u200c', 'bounding_box': [198, 336, 262, 356], 'confidence': 92}, {'text': 'උනා', 'bounding_box': [278, 342, 310, 359], 'confidence': 96}, {'text': 'අමරදේව', 'bounding_box': [326, 335, 398, 360], 'confidence': 93}, {'text': 'මාස්ටර්ව', 'bounding_box': [415, 335, 487, 356], 'confidence': 87}, {'text': 'හමුවෙන්න.', 'bounding_box': [10, 371, 98, 396], 'confidence': 94}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00ca7146-d38b-4b0a-b8fa-c97c99691338.json\n",
      "Annotations: [{'text': '1817-1818', 'bounding_box': [12, 16, 97, 32], 'confidence': 92}, {'text': 'ඌව', 'bounding_box': [113, 14, 147, 35], 'confidence': 95}, {'text': 'වෙල්ලස්ස', 'bounding_box': [163, 11, 243, 35], 'confidence': 94}, {'text': 'ජනතාව', 'bounding_box': [259, 14, 323, 32], 'confidence': 93}, {'text': 'බ්රිතාන්ය', 'bounding_box': [339, 11, 417, 32], 'confidence': 84}, {'text': 'රජයට', 'bounding_box': [432, 14, 484, 32], 'confidence': 96}, {'text': 'විරුද්ධව', 'bounding_box': [10, 47, 78, 72], 'confidence': 96}, {'text': 'නැගී', 'bounding_box': [94, 49, 130, 71], 'confidence': 96}, {'text': 'සිටීමෙන්\\u200c', 'bounding_box': [146, 47, 215, 68], 'confidence': 93}, {'text': 'බ්රිතාන්ය', 'bounding_box': [230, 47, 308, 68], 'confidence': 89}, {'text': 'හමුදා', 'bounding_box': [323, 50, 369, 72], 'confidence': 96}, {'text': 'විසින්\\u200c', 'bounding_box': [385, 47, 430, 68], 'confidence': 96}, {'text': 'එහි', 'bounding_box': [445, 49, 475, 68], 'confidence': 96}, {'text': 'වාසය', 'bounding_box': [10, 86, 57, 104], 'confidence': 88}, {'text': 'කළ', 'bounding_box': [72, 90, 102, 107], 'confidence': 96}, {'text': '10,000', 'bounding_box': [119, 88, 171, 106], 'confidence': 92}, {'text': 'පමණ', 'bounding_box': [187, 86, 235, 104], 'confidence': 96}, {'text': 'ජනතාවක්\\u200c', 'bounding_box': [251, 83, 333, 104], 'confidence': 96}, {'text': 'මරා', 'bounding_box': [350, 86, 380, 104], 'confidence': 96}, {'text': 'දමන', 'bounding_box': [396, 86, 436, 108], 'confidence': 96}, {'text': 'ලදහ.', 'bounding_box': [451, 90, 491, 108], 'confidence': 95}, {'text': 'සමහරු', 'bounding_box': [10, 122, 72, 143], 'confidence': 71}, {'text': 'ආගිය', 'bounding_box': [88, 121, 134, 143], 'confidence': 96}, {'text': 'අතක්\\u200c', 'bounding_box': [150, 119, 195, 143], 'confidence': 93}, {'text': 'නොමැතිවී', 'bounding_box': [211, 120, 292, 143], 'confidence': 92}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_000fb1af-e683-41a8-b2fb-3a2d5031b809.json\n",
      "Annotations: [{'text': 'කාටත්\\u200c', 'bounding_box': [10, 11, 62, 32], 'confidence': 94}, {'text': 'ළමා', 'bounding_box': [78, 14, 110, 35], 'confidence': 96}, {'text': 'සමයක්\\u200c', 'bounding_box': [126, 11, 185, 32], 'confidence': 96}, {'text': 'ඇත්තේ', 'bounding_box': [201, 11, 261, 35], 'confidence': 96}, {'text': 'ය.', 'bounding_box': [277, 18, 290, 32], 'confidence': 96}, {'text': 'ශ්රී', 'bounding_box': [310, 11, 338, 32], 'confidence': 96}, {'text': 'ලංකාවේ', 'bounding_box': [353, 12, 421, 35], 'confidence': 90}, {'text': 'බොහෝ', 'bounding_box': [437, 11, 501, 32], 'confidence': 96}, {'text': 'දෙනකුට', 'bounding_box': [10, 50, 77, 72], 'confidence': 95}, {'text': 'ගම්මානයක', 'bounding_box': [92, 48, 190, 68], 'confidence': 96}, {'text': 'ගත', 'bounding_box': [205, 54, 237, 68], 'confidence': 96}, {'text': 'වුණු', 'bounding_box': [252, 50, 286, 72], 'confidence': 96}, {'text': 'ළමා', 'bounding_box': [302, 50, 334, 71], 'confidence': 93}, {'text': 'වියක්\\u200c', 'bounding_box': [350, 47, 395, 68], 'confidence': 89}, {'text': 'අයත්\\u200c', 'bounding_box': [411, 47, 453, 71], 'confidence': 96}, {'text': 'ය.', 'bounding_box': [469, 54, 482, 68], 'confidence': 95}, {'text': 'එසේ', 'bounding_box': [10, 83, 48, 104], 'confidence': 96}, {'text': 'නම්\\u200c', 'bounding_box': [63, 84, 93, 104], 'confidence': 93}, {'text': 'වික්රමසිංහගේ', 'bounding_box': [109, 83, 232, 104], 'confidence': 83}, {'text': 'ළමා', 'bounding_box': [247, 86, 279, 107], 'confidence': 96}, {'text': 'සමය', 'bounding_box': [295, 86, 336, 104], 'confidence': 96}, {'text': 'සතු', 'bounding_box': [352, 90, 382, 108], 'confidence': 94}, {'text': 'සුවිශේෂී', 'bounding_box': [397, 83, 464, 108], 'confidence': 95}, {'text': 'ලක්ෂණය', 'bounding_box': [10, 119, 89, 143], 'confidence': 95}, {'text': 'කවරේද?', 'bounding_box': [105, 119, 175, 144], 'confidence': 96}, {'text': 'බොහෝ', 'bounding_box': [190, 119, 254, 140], 'confidence': 96}, {'text': 'දෙනාට', 'bounding_box': [269, 122, 324, 144], 'confidence': 96}, {'text': 'එසේ', 'bounding_box': [339, 119, 377, 140], 'confidence': 96}, {'text': 'සිතෙනු', 'bounding_box': [392, 121, 449, 144], 'confidence': 96}, {'text': 'ඇත.', 'bounding_box': [464, 126, 497, 143], 'confidence': 96}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00b44d86-7704-4eaa-8362-af4183e4e86c.json\n",
      "Annotations: [{'text': 'එදා', 'bounding_box': [10, 14, 39, 36], 'confidence': 96}, {'text': 'ඒ', 'bounding_box': [54, 11, 68, 32], 'confidence': 97}, {'text': 'කතාව', 'bounding_box': [83, 14, 134, 32], 'confidence': 96}, {'text': 'අවසන්\\u200c', 'bounding_box': [150, 11, 207, 35], 'confidence': 96}, {'text': 'වූයේ', 'bounding_box': [222, 11, 260, 36], 'confidence': 96}, {'text': 'ඔහුගේ', 'bounding_box': [276, 11, 333, 36], 'confidence': 96}, {'text': 'ඒ', 'bounding_box': [348, 11, 362, 32], 'confidence': 96}, {'text': 'කතාවත්\\u200c', 'bounding_box': [377, 11, 445, 32], 'confidence': 96}, {'text': 'සමගය.', 'bounding_box': [10, 50, 67, 68], 'confidence': 85}, {'text': 'කතාවේ', 'bounding_box': [87, 48, 149, 68], 'confidence': 96}, {'text': 'ඊලග', 'bounding_box': [165, 51, 207, 71], 'confidence': 96}, {'text': 'කොටස', 'bounding_box': [222, 50, 281, 68], 'confidence': 96}, {'text': 'පටන්\\u200c', 'bounding_box': [297, 47, 341, 68], 'confidence': 96}, {'text': 'ගන්නේ', 'bounding_box': [356, 47, 416, 68], 'confidence': 96}, {'text': 'ඒ', 'bounding_box': [431, 47, 445, 68], 'confidence': 96}, {'text': 'කතාවෙන්\\u200c', 'bounding_box': [10, 83, 90, 104], 'confidence': 96}, {'text': 'ටික', 'bounding_box': [105, 84, 135, 104], 'confidence': 93}, {'text': 'දවසකට', 'bounding_box': [151, 86, 219, 108], 'confidence': 96}, {'text': 'පසු', 'bounding_box': [234, 90, 260, 108], 'confidence': 96}, {'text': 'මා', 'bounding_box': [277, 86, 296, 104], 'confidence': 96}, {'text': 'රාජකාරී', 'bounding_box': [311, 84, 376, 104], 'confidence': 96}, {'text': 'කටයුත්තක්\\u200c', 'bounding_box': [391, 83, 485, 108], 'confidence': 96}, {'text': 'සදහා', 'bounding_box': [10, 126, 55, 144], 'confidence': 92}, {'text': 'සේවාදායකකු', 'bounding_box': [70, 119, 177, 144], 'confidence': 90}, {'text': 'හමුවීමට', 'bounding_box': [193, 120, 265, 144], 'confidence': 95}, {'text': 'ඔහුගේ', 'bounding_box': [282, 119, 339, 144], 'confidence': 96}, {'text': 'නිවසට', 'bounding_box': [354, 121, 411, 140], 'confidence': 96}, {'text': 'ගිය', 'bounding_box': [427, 121, 456, 140], 'confidence': 96}, {'text': \"'වේලාවකය.\", 'bounding_box': [10, 156, 99, 179], 'confidence': 82}, {'text': 'මා', 'bounding_box': [119, 158, 138, 176], 'confidence': 95}, {'text': 'එම', 'bounding_box': [153, 158, 181, 176], 'confidence': 96}, {'text': 'නිවෙසට', 'bounding_box': [196, 157, 264, 176], 'confidence': 96}, {'text': 'යන', 'bounding_box': [279, 162, 309, 176], 'confidence': 96}, {'text': 'විට', 'bounding_box': [324, 156, 351, 176], 'confidence': 96}, {'text': 'ඈත', 'bounding_box': [366, 162, 399, 179], 'confidence': 96}, {'text': 'තියාම', 'bounding_box': [414, 157, 463, 176], 'confidence': 96}, {'text': 'දුටුවේ.', 'bounding_box': [10, 192, 57, 216], 'confidence': 86}, {'text': 'ආලින්දය', 'bounding_box': [77, 191, 148, 216], 'confidence': 96}, {'text': 'පුරා', 'bounding_box': [163, 195, 193, 216], 'confidence': 96}, {'text': 'පුටු', 'bounding_box': [209, 194, 235, 216], 'confidence': 96}, {'text': 'කොට්ට', 'bounding_box': [251, 192, 310, 212], 'confidence': 93}, {'text': 'ගොඩක්\\u200c', 'bounding_box': [326, 191, 389, 212], 'confidence': 96}, {'text': 'ගසාගෙන', 'bounding_box': [405, 198, 482, 212], 'confidence': 96}, {'text': 'පුටුවක්\\u200c', 'bounding_box': [10, 227, 68, 252], 'confidence': 96}, {'text': 'උඩට', 'bounding_box': [84, 230, 123, 251], 'confidence': 96}, {'text': 'නැග', 'bounding_box': [139, 234, 175, 251], 'confidence': 96}, {'text': 'බිමට', 'bounding_box': [191, 228, 233, 248], 'confidence': 96}, {'text': 'පනින', 'bounding_box': [249, 229, 295, 248], 'confidence': 96}, {'text': 'අවුරුදු', 'bounding_box': [310, 230, 362, 252], 'confidence': 96}, {'text': '3ක්\\u200c', 'bounding_box': [379, 227, 405, 248], 'confidence': 96}, {'text': 'පමන', 'bounding_box': [421, 230, 465, 248], 'confidence': 97}, {'text': 'වූ', 'bounding_box': [480, 230, 493, 252], 'confidence': 96}, {'text': 'පනින', 'bounding_box': [10, 265, 56, 284], 'confidence': 95}, {'text': 'කුඩා', 'bounding_box': [71, 266, 108, 288], 'confidence': 95}, {'text': 'කොලු', 'bounding_box': [123, 270, 170, 288], 'confidence': 93}, {'text': 'ගැටයෙකි.', 'bounding_box': [185, 265, 260, 287], 'confidence': 92}, {'text': 'නිවස', 'bounding_box': [280, 265, 323, 284], 'confidence': 96}, {'text': 'ආසන්නයට', 'bounding_box': [339, 263, 430, 287], 'confidence': 96}, {'text': 'වෙත්ම', 'bounding_box': [445, 263, 500, 284], 'confidence': 93}, {'text': 'ඔහු', 'bounding_box': [11, 302, 41, 324], 'confidence': 96}, {'text': 'උඩ', 'bounding_box': [57, 302, 83, 323], 'confidence': 96}, {'text': 'සිට', 'bounding_box': [98, 301, 125, 320], 'confidence': 96}, {'text': 'පනින්නේ', 'bounding_box': [140, 299, 214, 320], 'confidence': 96}, {'text': 'පුලුන්\\u200c', 'bounding_box': [229, 299, 274, 324], 'confidence': 96}, {'text': 'පිරවූ', 'bounding_box': [289, 301, 328, 324], 'confidence': 93}, {'text': 'ටෙඩියෙකු', 'bounding_box': [344, 300, 424, 324], 'confidence': 92}, {'text': 'මතට', 'bounding_box': [440, 302, 483, 320], 'confidence': 96}, {'text': 'බව', 'bounding_box': [10, 338, 38, 356], 'confidence': 93}, {'text': 'හෙදින්ම', 'bounding_box': [54, 335, 126, 360], 'confidence': 91}, {'text': 'මා', 'bounding_box': [143, 338, 162, 356], 'confidence': 96}, {'text': 'දුටුවෙමි.', 'bounding_box': [177, 336, 238, 360], 'confidence': 96}, {'text': 'මේ', 'bounding_box': [258, 336, 283, 356], 'confidence': 96}, {'text': 'අනිවාර්යෙන්ම', 'bounding_box': [298, 335, 412, 359], 'confidence': 96}, {'text': 'පොඩි', 'bounding_box': [428, 336, 471, 356], 'confidence': 96}, {'text': 'තේන්සිනා', 'bounding_box': [10, 371, 90, 392], 'confidence': 84}, {'text': 'කෙනෙක්\\u200c', 'bounding_box': [106, 371, 177, 392], 'confidence': 96}, {'text': 'බව', 'bounding_box': [193, 374, 221, 392], 'confidence': 96}, {'text': 'මම', 'bounding_box': [238, 374, 266, 392], 'confidence': 96}, {'text': 'ඒ', 'bounding_box': [274, 365, 287, 403], 'confidence': 96}, {'text': 'ඉරියව්\\u200c', 'bounding_box': [310, 372, 362, 395], 'confidence': 96}, {'text': 'වලින්\\u200c', 'bounding_box': [378, 371, 423, 395], 'confidence': 96}, {'text': 'දුටුවෙමි.', 'bounding_box': [438, 372, 499, 396], 'confidence': 95}, {'text': 'අවුරුදු', 'bounding_box': [10, 410, 62, 432], 'confidence': 96}, {'text': '3ක්\\u200c', 'bounding_box': [79, 407, 105, 428], 'confidence': 96}, {'text': 'වත්\\u200c', 'bounding_box': [121, 407, 151, 428], 'confidence': 96}, {'text': 'නොවූ', 'bounding_box': [167, 410, 212, 432], 'confidence': 96}, {'text': 'කොලු', 'bounding_box': [228, 414, 275, 432], 'confidence': 96}, {'text': 'ගැටයා', 'bounding_box': [290, 410, 342, 431], 'confidence': 93}, {'text': 'ටෙඩියා', 'bounding_box': [358, 408, 415, 428], 'confidence': 91}, {'text': 'කුඩුපට්ටම', 'bounding_box': [10, 444, 96, 468], 'confidence': 74}, {'text': 'කර', 'bounding_box': [112, 447, 141, 464], 'confidence': 93}, {'text': 'දැමීමෙ', 'bounding_box': [156, 444, 210, 468], 'confidence': 90}, {'text': 'මහා', 'bounding_box': [227, 446, 262, 464], 'confidence': 93}, {'text': 'මෙහෙයුමකය', 'bounding_box': [277, 446, 387, 468], 'confidence': 92}, {'text': 'විටින්\\u200c', 'bounding_box': [403, 443, 447, 464], 'confidence': 96}, {'text': 'විට', 'bounding_box': [462, 444, 489, 464], 'confidence': 96}, {'text': 'රූපවාහිනි', 'bounding_box': [10, 481, 93, 503], 'confidence': 93}, {'text': 'රෙස්ලින්\\u200c', 'bounding_box': [108, 479, 176, 503], 'confidence': 94}, {'text': 'තරගවලින්\\u200c', 'bounding_box': [191, 479, 280, 503], 'confidence': 93}, {'text': 'අහුලාගත්තා', 'bounding_box': [295, 479, 396, 504], 'confidence': 88}, {'text': 'යැයි', 'bounding_box': [412, 481, 443, 503], 'confidence': 96}, {'text': 'සිතිය', 'bounding_box': [459, 481, 502, 500], 'confidence': 96}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00edf6db-a5e1-408c-b651-d9f7ec838fde.json\n",
      "Annotations: [{'text': 'මගේ', 'bounding_box': [11, 11, 51, 32], 'confidence': 96}, {'text': 'ලිස්ට්\\u200c', 'bounding_box': [66, 11, 107, 35], 'confidence': 95}, {'text': 'එක්\\u200c', 'bounding_box': [123, 11, 164, 32], 'confidence': 96}, {'text': 'නම්\\u200c', 'bounding_box': [180, 12, 210, 32], 'confidence': 96}, {'text': 'ඔතන', 'bounding_box': [227, 14, 273, 32], 'confidence': 95}, {'text': 'තව', 'bounding_box': [288, 14, 317, 32], 'confidence': 95}, {'text': 'එකක්\\u200c', 'bounding_box': [333, 11, 381, 32], 'confidence': 96}, {'text': 'තියාගෙන', 'bounding_box': [397, 13, 474, 32], 'confidence': 96}, {'text': 'ඉන්නේ.', 'bounding_box': [10, 47, 67, 71], 'confidence': 93}, {'text': 'ප්රේමය්\\u200c', 'bounding_box': [85, 47, 150, 68], 'confidence': 91}, {'text': 'බලාපොරොත්තුව', 'bounding_box': [165, 47, 302, 72], 'confidence': 93}, {'text': 'සංතුෂ්ඨිය', 'bounding_box': [318, 47, 397, 72], 'confidence': 76}, {'text': 'කියලා', 'bounding_box': [413, 49, 463, 71], 'confidence': 96}, {'text': 'මොන', 'bounding_box': [10, 86, 57, 104], 'confidence': 96}, {'text': 'අයිසිං', 'bounding_box': [72, 85, 120, 107], 'confidence': 96}, {'text': 'දැම්මත්\\u200c', 'bounding_box': [135, 83, 195, 108], 'confidence': 92}, {'text': 'ඕක', 'bounding_box': [212, 84, 243, 104], 'confidence': 96}, {'text': 'ඇතුලෙ', 'bounding_box': [258, 90, 316, 108], 'confidence': 96}, {'text': 'තියෙන්නෙ', 'bounding_box': [331, 83, 414, 104], 'confidence': 95}, {'text': 'අපි', 'bounding_box': [429, 85, 454, 107], 'confidence': 96}, {'text': 'හැමදාම', 'bounding_box': [10, 122, 75, 144], 'confidence': 93}, {'text': 'රහවිදින', 'bounding_box': [91, 120, 161, 144], 'confidence': 66}, {'text': 'ඒ', 'bounding_box': [176, 119, 190, 140], 'confidence': 96}, {'text': 'කේක්\\u200c', 'bounding_box': [205, 119, 250, 140], 'confidence': 93}, {'text': 'කෑල්ලමයි.', 'bounding_box': [266, 119, 345, 143], 'confidence': 86}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00f17190-69bd-459e-8e27-96a3b85c1d6f.json\n",
      "Annotations: [{'text': 'කාලෙන්\\u200c', 'bounding_box': [10, 11, 74, 35], 'confidence': 96}, {'text': 'කාලෙට', 'bounding_box': [89, 14, 149, 35], 'confidence': 93}, {'text': 'හිටිද', 'bounding_box': [164, 12, 204, 36], 'confidence': 44}, {'text': 'එහෙම', 'bounding_box': [219, 14, 274, 32], 'confidence': 96}, {'text': 'නැද්ද', 'bounding_box': [289, 11, 329, 36], 'confidence': 96}, {'text': 'කියන්න', 'bounding_box': [345, 11, 408, 32], 'confidence': 96}, {'text': 'මම', 'bounding_box': [424, 14, 452, 32], 'confidence': 96}, {'text': 'දන්නෙ', 'bounding_box': [10, 47, 62, 72], 'confidence': 96}, {'text': 'නෑ.', 'bounding_box': [78, 54, 99, 71], 'confidence': 96}, {'text': 'එදිනෙදා', 'bounding_box': [119, 49, 184, 72], 'confidence': 96}, {'text': 'යන', 'bounding_box': [200, 54, 230, 68], 'confidence': 96}, {'text': 'එන', 'bounding_box': [245, 50, 275, 68], 'confidence': 97}, {'text': 'හැම', 'bounding_box': [290, 50, 326, 71], 'confidence': 97}, {'text': 'තැනකදීම', 'bounding_box': [341, 49, 419, 72], 'confidence': 96}, {'text': 'හමුවෙන', 'bounding_box': [10, 86, 81, 108], 'confidence': 74}, {'text': 'අය', 'bounding_box': [97, 90, 122, 107], 'confidence': 97}, {'text': 'මට', 'bounding_box': [139, 86, 166, 104], 'confidence': 96}, {'text': 'දක්වන', 'bounding_box': [181, 83, 238, 108], 'confidence': 89}, {'text': 'ආදරය', 'bounding_box': [253, 87, 305, 108], 'confidence': 96}, {'text': 'කරුණාව', 'bounding_box': [321, 86, 394, 107], 'confidence': 96}, {'text': '6ළෙන්ගතුකම', 'bounding_box': [10, 119, 113, 144], 'confidence': 53}, {'text': 'නිසා', 'bounding_box': [128, 121, 163, 140], 'confidence': 96}, {'text': 'ඔවුන්\\u200c', 'bounding_box': [179, 119, 224, 144], 'confidence': 96}, {'text': 'අතර', 'bounding_box': [239, 123, 279, 143], 'confidence': 96}, {'text': 'ආදරණීය', 'bounding_box': [294, 120, 367, 144], 'confidence': 96}, {'text': 'චරිතයක්\\u200c', 'bounding_box': [383, 119, 456, 140], 'confidence': 96}, {'text': 'කියලනම්\\u200c', 'bounding_box': [10, 156, 86, 179], 'confidence': 96}, {'text': 'කියන්න', 'bounding_box': [101, 155, 164, 176], 'confidence': 96}, {'text': 'පුළුවන්\\u200c.', 'bounding_box': [179, 155, 239, 180], 'confidence': 96}]\n",
      "Word-level accuracy: 0.4754 (135/284)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4753521126760563"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_decode(logits):\n",
    "    # logits: (T, N, C) where T = time, N = batch*words, C = num_classes\n",
    "    # Returns: list of lists of predicted indices (one per word)\n",
    "    probs = logits.softmax(2)\n",
    "    max_probs, max_indices = probs.max(2)  # (T, N)\n",
    "    max_indices = max_indices.cpu().numpy().T  # (N, T)\n",
    "    decoded = []\n",
    "    for seq in max_indices:\n",
    "        # Collapse repeats and remove blanks (assume blank=0)\n",
    "        prev = -1\n",
    "        out = []\n",
    "        for idx in seq:\n",
    "            if idx != prev and idx != 0:\n",
    "                out.append(idx)\n",
    "            prev = idx\n",
    "        decoded.append(out)\n",
    "    return decoded\n",
    "\n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            images, boxes_list, text_targets_list, target_lengths_list = batch\n",
    "            images = images.to(device)\n",
    "            recog_logits = model(images, boxes_list)\n",
    "            if recog_logits is None or recog_logits.size(1) == 0:\n",
    "                continue\n",
    "            decoded = greedy_decode(recog_logits)\n",
    "            # Flatten ground truth targets\n",
    "            gt_texts = []\n",
    "            for text_list in text_targets_list:\n",
    "                for seq in text_list:\n",
    "                    gt_texts.append(sequence_to_text(seq.cpu().numpy()))\n",
    "            # Convert predictions to text\n",
    "            pred_texts = [sequence_to_text(seq) for seq in decoded]\n",
    "            # Compare predictions and ground truth\n",
    "            for gt, pred in zip(gt_texts, pred_texts):\n",
    "                if gt == pred:\n",
    "                    correct_words += 1\n",
    "                total_words += 1\n",
    "    acc = correct_words / total_words if total_words > 0 else 0\n",
    "    print(f\"Word-level accuracy: {acc:.4f} ({correct_words}/{total_words})\")\n",
    "    return acc\n",
    "\n",
    "# Example usage after training:\n",
    "compute_accuracy(model, dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
