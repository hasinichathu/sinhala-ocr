{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "\n",
    "def insert_into_square_canvas(\n",
    "        in_path: str | Path,\n",
    "        out_path: str | Path,\n",
    "        canvas_side: int = 512,\n",
    "        bg_color: tuple[int, int, int] = (255, 255, 255)  # white\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Open `in_path`, letter-box it into a `canvas_side`×`canvas_side` square and\n",
    "    save to `out_path`.\n",
    "\n",
    "    The original aspect ratio is preserved; no cropping or stretching occurs.\n",
    "    \"\"\"\n",
    "    img = Image.open(in_path).convert(\"RGB\")\n",
    "\n",
    "    # --- resize keeping aspect ratio ----------------------------------------\n",
    "    img.thumbnail((canvas_side, canvas_side), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # --- create square canvas and centre the image --------------------------\n",
    "    canvas = Image.new(\"RGB\", (canvas_side, canvas_side), color=bg_color)\n",
    "    x = (canvas_side - img.width)  // 2\n",
    "    y = (canvas_side - img.height) // 2\n",
    "    canvas.paste(img, (x, y))\n",
    "\n",
    "    # --- save ---------------------------------------------------------------\n",
    "    canvas.save(out_path, quality=95)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinhala_preprocess.py\n",
    "import cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "# -------------------------------------------------\n",
    "def preprocess_for_ocr(\n",
    "    img_path: str | Path,\n",
    "    out_path: str | Path,\n",
    "    dpi_target: int = 300,\n",
    "    debug: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Load image → run the 8-step pipeline → return numpy array.\n",
    "    If `out_path` is given, save the final result (PNG, 1-channel).\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(img_path))\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read {img_path}\")\n",
    "\n",
    "    # ① estimate current DPI (fallback to resize longest edge)\n",
    "    h, w = img.shape[:2]\n",
    "    LONG_EDGE_TARGET = 3508   # A4 @300 DPI ≈ 3508 px\n",
    "    if max(h, w) < LONG_EDGE_TARGET:\n",
    "        scale = LONG_EDGE_TARGET / max(h, w)\n",
    "        img = cv2.resize(img, None, fx=scale, fy=scale,\n",
    "                         interpolation=cv2.INTER_CUBIC)\n",
    "        if debug: print(f\"Upscaled by {scale:.2f}× to ~300 DPI\")\n",
    "\n",
    "    # ② grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ③ CLAHE (clip=2.0, tile=8×8) \n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    gray = clahe.apply(gray)\n",
    "\n",
    "    # ④ adaptive threshold (Gaussian, invert for white text)\n",
    "    bin_img = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, blockSize=51, C=15)\n",
    "\n",
    "    # ⑤ median blur (salt-and-pepper)\n",
    "    bin_img = cv2.medianBlur(bin_img, 3)\n",
    "\n",
    "    # ⑥ closing tiny gaps\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 1))\n",
    "    bin_img = cv2.morphologyEx(bin_img, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "\n",
    "    # ⑧ crop black borders (optional; comment out if you keep margins)\n",
    "    contours, _ = cv2.findContours(\n",
    "        bin_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        x, y, w2, h2 = cv2.boundingRect(\n",
    "            np.concatenate(contours))      # union of all\n",
    "        if w2 > 0 and h2 > 0:\n",
    "            bin_img = bin_img[y:y+h2, x:x+w2]\n",
    "\n",
    "    if out_path:\n",
    "        cv2.imwrite(str(out_path), bin_img)\n",
    "\n",
    "    return bin_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # ⑦ deskew\n",
    "    # coords = np.column_stack(np.where(bin_img > 0))\n",
    "    # if coords.size:\n",
    "    #     angle = cv2.minAreaRect(coords)[-1]\n",
    "    #     angle = -(90 + angle) if angle < -45 else -angle\n",
    "    #     if abs(angle) > 0.5:          # ignore almost-straight\n",
    "    #         h2, w2 = bin_img.shape\n",
    "    #         M = cv2.getRotationMatrix2D((w2 / 2, h2 / 2), angle, 1.0)\n",
    "    #         bin_img = cv2.warpAffine(\n",
    "    #             bin_img, M, (w2, h2),\n",
    "    #             flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    #         if debug: print(f\"Deskewed by {angle:.2f}°\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding boxes saved to: D:\\python\\data\\json\\page_5_1.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_5_1_boxed.jpg\n",
      "Bounding boxes saved to: D:\\python\\data\\json\\page_6_1.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_6_1_boxed.jpg\n",
      "Bounding boxes saved to: D:\\python\\data\\json\\page_6_2+6.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_6_2+6_boxed.jpg\n",
      "Bounding boxes saved to: D:\\python\\data\\json\\page_6_3.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_6_3_boxed.jpg\n",
      "Bounding boxes saved to: D:\\python\\data\\json\\page_6_4.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_6_4_boxed.jpg\n",
      "Bounding boxes saved to: D:\\python\\data\\json\\page_6_5.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_6_5_boxed.jpg\n",
      "Bounding boxes saved to: D:\\python\\data\\json\\page_6_7.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_6_7_boxed.jpg\n",
      "Bounding boxes saved to: D:\\python\\data\\json\\page_6_8.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_6_8_boxed.jpg\n",
      "Bounding boxes saved to: D:\\python\\data\\json\\page_7_1.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_7_1_boxed.jpg\n",
      "Bounding boxes saved to: D:\\python\\data\\json\\page_7_2.json\n",
      "Image with bounding boxes saved to: D:\\python\\data\\boxed\\page_7_2_boxed.jpg\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to load image: D:\\python\\data\\images\\page_8_3.png",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m output_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m     \u001b[38;5;66;03m# Replace with your output folder\u001b[39;00m\n\u001b[0;32m     76\u001b[0m output_boxed \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mboxed\u001b[39m\u001b[38;5;124m\"\u001b[39m     \u001b[38;5;66;03m# Replace with your output folder\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_boxed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 70\u001b[0m, in \u001b[0;36mprocess_images\u001b[1;34m(folder_path, output_folder_json, output_folder_boxed)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as JSON file already exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_json_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[43mextract_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_json_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_boxed_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 14\u001b[0m, in \u001b[0;36mextract_bounding_boxes\u001b[1;34m(image_path, output_json_path, output_boxed_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m h, w, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# Get image dimensions\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Perform OCR and extract data at the word level\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to load image: D:\\python\\data\\images\\page_8_3.png"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set Tesseract executable path (required for Windows users)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Function to extract word-level bounding boxes and text using Tesseract\n",
    "def extract_bounding_boxes(image_path, output_json_path, output_boxed_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Unable to load image: {image_path}\")\n",
    "    h, w, _ = image.shape  # Get image dimensions\n",
    "\n",
    "    # Perform OCR and extract data at the word level\n",
    "    data = pytesseract.image_to_data(image, lang='sin', output_type=pytesseract.Output.DICT)\n",
    "\n",
    "    bounding_box_data = []\n",
    "    n_boxes = len(data[\"level\"])\n",
    "    for i in range(n_boxes):\n",
    "        text = data[\"text\"][i].strip()\n",
    "        # Skip empty text results\n",
    "        if text != \"\":\n",
    "            left = data[\"left\"][i]\n",
    "            top = data[\"top\"][i]\n",
    "            width_box = data[\"width\"][i]\n",
    "            height_box = data[\"height\"][i]\n",
    "            # Create bounding box as [x_min, y_min, x_max, y_max]\n",
    "            bbox = [left, top, left + width_box, top + height_box]\n",
    "            bounding_box_data.append({\n",
    "                \"text\": text,\n",
    "                \"bounding_box\": bbox,\n",
    "                \"confidence\": data[\"conf\"][i]\n",
    "            })\n",
    "\n",
    "            # Draw bounding box on the image for visualization\n",
    "            cv2.rectangle(image, (left, top), (left + width_box, top + height_box), (0, 255, 0), 2)\n",
    "            cv2.putText(image, text, (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    # Save bounding box data to a JSON file\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(bounding_box_data, json_file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Bounding boxes saved to: {output_json_path}\")\n",
    "\n",
    "    # Save the image with bounding boxes to a file \n",
    "    cv2.imwrite(output_boxed_path, image)\n",
    "    print(f\"Image with bounding boxes saved to: {output_boxed_path}\")\n",
    "\n",
    "# Main function to process images in a folder\n",
    "def process_images(folder_path, output_folder_json, output_folder_boxed):\n",
    "    if not os.path.exists(output_folder_json):\n",
    "        os.makedirs(output_folder_json)\n",
    "    if not os.path.exists(output_folder_boxed):\n",
    "        os.makedirs(output_folder_boxed)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Process common image file formats\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            output_json_path = os.path.join(output_folder_json, f\"{os.path.splitext(filename)[0]}.json\")\n",
    "            output_boxed_path = os.path.join(output_folder_boxed, f\"{os.path.splitext(filename)[0]}_boxed.jpg\")\n",
    "            \n",
    "            # Check if the JSON file already exists\n",
    "            if os.path.exists(output_json_path):\n",
    "                print(f\"Skipping {filename} as JSON file already exists: {output_json_path}\")\n",
    "                continue\n",
    "            \n",
    "            extract_bounding_boxes(image_path, output_json_path, output_boxed_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_21804\\4286088037.py\", line 3, in <module>\n",
      "    from kraken import binarization        # nlbin\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\kraken\\binarization.py\", line 32, in <module>\n",
      "    from kraken.lib.util import array2pil, get_im_str, is_bitonal, pil2array\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\kraken\\lib\\util.py\", line 13, in <module>\n",
      "    from kraken.containers import BBoxLine\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\kraken\\containers.py\", line 30, in <module>\n",
      "    from kraken.lib.segmentation import compute_polygon_section\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\kraken\\lib\\segmentation.py\", line 40, in <module>\n",
      "    from skimage.morphology import skeletonize\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skimage\\morphology\\__init__.py\", line 9, in <module>\n",
      "    from .binary import binary_closing, binary_dilation, binary_erosion, binary_opening\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skimage\\morphology\\binary.py\", line 9, in <module>\n",
      "    from .misc import default_footprint\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skimage\\morphology\\misc.py\", line 9, in <module>\n",
      "    from .._shared._dependency_checks import is_wasm\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skimage\\_shared\\_dependency_checks.py\", line 5, in <module>\n",
      "    has_mpl = is_installed(\"matplotlib\", \">=3.3\")\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skimage\\_shared\\version_requirements.py\", line 58, in is_installed\n",
      "    actver = get_module_version(name)\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skimage\\_shared\\version_requirements.py\", line 32, in get_module_version\n",
      "    mod = __import__(module_name, fromlist=[module_name.rpartition('.')[-1]])\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\__init__.py\", line 174, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\colors.py\", line 57, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\ticker.py\", line 143, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\matplotlib\\transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from kraken import binarization        # nlbin\n",
    "\n",
    "\n",
    "def binarize_and_save(src_path: Path, dst_path: Path) -> None:\n",
    "    img_pil   = Image.open(src_path).convert(\"RGB\")   # <-- FIX: create PIL.Image\n",
    "    img_clean = binarization.nlbin(img_pil)           # pass the Image, not a str\n",
    "    img_clean.save(dst_path, format=\"PNG\", optimize=True)\n",
    "    print(f\"✔ {dst_path.name} saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ page_7_1.png saved\n",
      "✔ page_7_2.png saved\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'insert_into_square_canvas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     fp_path \u001b[38;5;241m=\u001b[39m Path(fp)  \u001b[38;5;66;03m# Convert string to Path object\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     out_fp \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(canvased_img_dir, fp_path\u001b[38;5;241m.\u001b[39mstem \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     \u001b[43minsert_into_square_canvas\u001b[49m(fp_path, out_fp)\n\u001b[0;32m     20\u001b[0m input_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# Replace with your folder containing images\u001b[39;00m\n\u001b[0;32m     21\u001b[0m output_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m     \u001b[38;5;66;03m# Replace with your output folder\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'insert_into_square_canvas' is not defined"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "unedited_img_dir =  r\"D:\\python\\data\\images_unedited\" \n",
    "binarized_img_dir =  r\"D:\\python\\data\\binarized_images\" \n",
    "canvased_img_dir =  r\"D:\\python\\data\\images\" \n",
    "os.makedirs(canvased_img_dir, exist_ok=True)\n",
    "\n",
    "for fp in glob(f\"{unedited_img_dir}/*.*\"):\n",
    "    fp_path = Path(fp)  # Convert string to Path object\n",
    "    out_fp = Path(binarized_img_dir) / f\"{fp_path.stem}.png\"  # Ensure out_fp is a Path object\n",
    "    binarize_and_save(fp_path, out_fp)  # Pass Path objects directly to the function\n",
    "\n",
    "\n",
    "for fp in glob(f\"{binarized_img_dir}/*.*\"):\n",
    "    fp_path = Path(fp)  # Convert string to Path object\n",
    "    out_fp = os.path.join(canvased_img_dir, fp_path.stem + \".png\")\n",
    "    insert_into_square_canvas(fp_path, out_fp)\n",
    "\n",
    "input_folder = r\"D:\\python\\data\\images\"   # Replace with your folder containing images\n",
    "output_json = r\"D:\\python\\data\\json\"     # Replace with your output folder\n",
    "output_boxed = r\"D:\\python\\data\\boxed\"     # Replace with your output folder\n",
    "process_images(input_folder, output_json, output_boxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Define the Character Set and Helper Functions\n",
    "# ------------------------------\n",
    "\n",
    "# In this example, we use a simple character set: lowercase letters and space.\n",
    "# For CTC loss, index 0 is reserved as the blank token.\n",
    "charset = ['අ', 'ආ', 'ඇ', 'ඈ', 'ඉ', 'ඊ', 'උ', 'ඌ', 'එ', 'ඒ', 'ඔ', 'ඕ', 'ඓ', 'ඖ',\n",
    "  'ක', 'කා', 'කැ', 'කෑ', 'කි', 'කී', 'කු', 'කූ', 'කෙ', 'කේ', 'කො', 'කෝ', 'කෛ', 'කෞ',\n",
    "  'ඛ', 'ඛා', 'ඛැ', 'ඛෑ', 'ඛි', 'ඛී', 'ඛු', 'ඛූ', 'ඛෙ', 'ඛේ', 'ඛො', 'ඛෝ', 'ඛෛ', 'ඛෞ',\n",
    "  'ග', 'ගා', 'ගැ', 'ගෑ', 'ගි', 'ගී', 'ගු', 'ගූ', 'ගෙ', 'ගේ', 'ගො', 'ගෝ', 'ගෛ', 'ගෞ',\n",
    "  'ච', 'චා', 'චැ', 'චෑ', 'චි', 'චී', 'චු', 'චූ', 'චෙ', 'චේ', 'චො', 'චෝ', 'චෛ', 'චෞ',\n",
    "  'ජ', 'ජා', 'ජැ', 'ජෑ', 'ජි', 'ජී', 'ජු', 'ජූ', 'ජෙ', 'ජේ', 'ජො', 'ජෝ', 'ජෛ', 'ජෞ',\n",
    "  'ට', 'ටා', 'ටැ', 'ටෑ', 'ටි', 'ටී', 'ටු', 'ටූ', 'ටෙ', 'ටේ', 'ටො', 'ටෝ', 'ටෛ', 'ටෞ',\n",
    "  'ඩ', 'ඩා', 'ඩැ', 'ඩෑ', 'ඩි', 'ඩී', 'ඩු', 'ඩූ', 'ඩෙ', 'ඩේ', 'ඩො', 'ඩෝ', 'ඩෛ', 'ඩෞ',\n",
    "  'ණ', 'ණා', 'ණැ', 'ණෑ', 'ණි', 'ණී', 'ණු', 'ණූ', 'ණෙ', 'ණේ', 'ණො', 'ණෝ', 'ණෛ', 'ණෞ',\n",
    "  'ත', 'තා', 'තැ', 'තෑ', 'ති', 'තී', 'තු', 'තූ', 'තෙ', 'තේ', 'තො', 'තෝ', 'තෛ', 'තෞ',\n",
    "  'ද', 'දා', 'දැ', 'දෑ', 'දි', 'දී', 'දු', 'දූ', 'දෙ', 'දේ', 'දො', 'දෝ', 'දෛ', 'දෞ',\n",
    "  'න', 'නා', 'නැ', 'නෑ', 'නි', 'නී', 'නු', 'නූ', 'නෙ', 'නේ', 'නො', 'නෝ', 'නෛ', 'නෞ',\n",
    "  'ප', 'පා', 'පැ', 'පෑ', 'පි', 'පී', 'පු', 'පූ', 'පෙ', 'පේ', 'පො', 'පෝ', 'පෛ', 'පෞ',\n",
    "  'බ', 'බා', 'බැ', 'බෑ', 'බි', 'බී', 'බු', 'බූ', 'බෙ', 'බේ', 'බො', 'බෝ', 'බෛ', 'බෞ',\n",
    "  'ම', 'මා', 'මැ', 'මෑ', 'මි', 'මී', 'මු', 'මූ', 'මෙ', 'මේ', 'මො', 'මෝ', 'මෛ', 'මෞ',\n",
    "  'ය', 'යා', 'යැ', 'යෑ', 'යි', 'යී', 'යු', 'යූ', 'යෙ', 'යේ', 'යො', 'යෝ', 'යෛ', 'යෞ',\n",
    "  'ර', 'රා', 'රැ', 'රෑ', 'රි', 'රී', 'රු', 'රූ', 'රෙ', 'රේ', 'රො', 'රෝ', 'රෛ', 'රෞ',\n",
    "  'ල', 'ලා', 'ලැ', 'ලෑ', 'ලි', 'ලී', 'ලු', 'ලූ', 'ලෙ', 'ලේ', 'ලො', 'ලෝ', 'ලෛ', 'ලෞ',\n",
    "  'ව', 'වා', 'වැ', 'වෑ', 'වි', 'වී', 'වු', 'වූ', 'වෙ', 'වේ', 'වො', 'වෝ', 'වෛ', 'වෞ',\n",
    "  'ශ', 'ශා', 'ශැ', 'ශෑ', 'ශි', 'ශී', 'ශු', 'ශූ', 'ශෙ', 'ශේ', 'ශො', 'ශෝ', 'ශෛ', 'ශෞ',\n",
    "  'ෂ', 'ෂා', 'ෂැ', 'ෂෑ', 'ෂි', 'ෂී', 'ෂු', 'ෂූ', 'ෂෙ', 'ෂේ', 'ෂො', 'ෂෝ', 'ෂෛ', 'ෂෞ',\n",
    "  'ස', 'සා', 'සැ', 'සෑ', 'සි', 'සී', 'සු', 'සූ', 'සෙ', 'සේ', 'සො', 'සෝ', 'සෛ', 'සෞ',\n",
    "  'හ', 'හා', 'හැ', 'හෑ', 'හි', 'හී', 'හු', 'හූ', 'හෙ', 'හේ', 'හො', 'හෝ', 'හෛ', 'හෞ',\n",
    "  'ළ', 'ළා', 'ළැ', 'ළෑ', 'ළි', 'ළී', 'ළු', 'ළූ', 'ළෙ', 'ළේ', 'ළො', 'ළෝ', 'ළෛ', 'ළෞ','ක්‍ර', 'ක්‍රි', 'ක්‍රී', 'ක්‍රා', 'ක්‍රැ', 'ක්‍රෑ',\n",
    "  'ප්‍ර', 'ප්‍රි', 'ප්‍රී', 'ප්‍රා',\n",
    "  'ද්‍ර', 'ද්‍රි', 'ද්‍රී', 'ද්‍රෝ',\n",
    "  'ත්‍ර', 'ත්‍රි', 'ත්‍රී', 'ත්‍රැ', 'ත්‍රෑ',\n",
    "  'ශ්‍ර', 'ශ්‍රි', 'ශ්‍රී', 'ශ්‍රැ', 'ශ්‍රෑ'\n",
    "            \n",
    "]\n",
    "char2idx = {c: i + 1 for i, c in enumerate(charset)}  # map characters to indices (starting at 1)\n",
    "num_classes = len(charset) + 1  # +1 for CTC blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text):\n",
    "    \"\"\"Convert a text string to a list of indices based on the charset.\"\"\"\n",
    "    return [char2idx[c] for c in text if c in char2idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2. Define a Custom Dataset for Multi-Word OCR\n",
    "# ------------------------------\n",
    "\n",
    "class OCRMultiWordDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, transform=None):\n",
    "        \"\"\"\n",
    "        images_dir: Directory of image files.\n",
    "        annotations_dir: Directory of JSON annotation files – one per image.\n",
    "            Each JSON file is expected to have the following structure:\n",
    "            [\n",
    "                {\n",
    "                    \"text\": \"word1\",\n",
    "                    \"bounding_box\": [x_min, y_min, x_max, y_max],\n",
    "                    \"confidence\": confidence_value\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        transform: Optional transform to be applied to a PIL image or NumPy array.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = self.image_files[idx]\n",
    "            img_path = os.path.join(self.images_dir, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image at {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = transforms.ToTensor()(image)\n",
    "\n",
    "            # Construct the annotation path with the \"_boxed\" suffix\n",
    "            annotation_name = f\"{os.path.splitext(img_name)[0]}.json\"\n",
    "            annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "\n",
    "            # Check if the annotation file exists\n",
    "            if not os.path.exists(annotation_path):\n",
    "                raise FileNotFoundError(f\"Annotation file not found: {annotation_path}\")\n",
    "\n",
    "            # Specify UTF-8 encoding when opening the JSON file\n",
    "            with open(annotation_path, 'r', encoding='utf-8') as f:\n",
    "                annotations = json.load(f)\n",
    "\n",
    "            # Debug: Print the loaded JSON file path and its content\n",
    "            print(f\"Loaded JSON file: {annotation_path}\")\n",
    "            print(f\"Annotations: {annotations}\")\n",
    "\n",
    "            # Process annotations (as in your existing code)\n",
    "            boxes = []\n",
    "            text_targets = []\n",
    "            target_lengths = []\n",
    "\n",
    "            _, H, W = image.shape\n",
    "            for ann in annotations:\n",
    "                bbox = ann[\"bounding_box\"]\n",
    "                bbox_norm = [bbox[0] / W, bbox[1] / H, bbox[2] / W, bbox[3] / H]\n",
    "                boxes.append(bbox_norm)\n",
    "                text = ann[\"text\"].strip().lower()\n",
    "                seq = text_to_sequence(text)\n",
    "                text_targets.append(torch.tensor(seq, dtype=torch.long))\n",
    "                target_lengths.append(len(seq))\n",
    "\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            return image, boxes, text_targets, target_lengths\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {img_name}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 3. Create a Custom Collate Function\n",
    "# ------------------------------\n",
    "\n",
    "def ocr_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This collate function takes a list of samples from OCRMultiWordDataset.\n",
    "    Each sample: (image, boxes, text_targets, target_lengths)\n",
    "    It stacks the images into a batch tensor and keeps the others in lists.\n",
    "    \"\"\"\n",
    "    images = [item[0] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    boxes_list = [item[1] for item in batch]          # List of tensors (sizes vary per image)\n",
    "    text_targets_list = [item[2] for item in batch]     # List of lists (each inner list may have varying number)\n",
    "    target_lengths_list = [item[3] for item in batch]   # List of lists of ints\n",
    "\n",
    "    return images, boxes_list, text_targets_list, target_lengths_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 4. Define the Recognition Network\n",
    "# ------------------------------\n",
    "\n",
    "class TextRecognizer(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TextRecognizer, self).__init__()\n",
    "        # A simple CNN backbone.\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),   # (B, 64, H, W)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                           # (B, 64, H/2, W/2)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (B, 128, H/2, W/2)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)                            # (B, 128, H/4, W/4)\n",
    "        )\n",
    "        # After CNN, average pool over the height dimension to convert feature map into a sequence.\n",
    "        # In our example, we assume the crop has been resized to (32, 128) (height, width).\n",
    "        # After 2 pooling layers: height becomes 32/4 = 8, width becomes 128/4 = 32.\n",
    "        self.lstm = nn.LSTM(128, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(256 * 2, num_classes)  # bidirectional LSTM outputs hidden_dim*2 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (B, 3, 32, 128)\n",
    "        Returns:\n",
    "            Outputs of shape (T, B, num_classes), where T is the sequence length (here, 32).\n",
    "        \"\"\"\n",
    "        features = self.cnn(x)  # (B, 128, 8, 32)\n",
    "        # Average over the height dimension to form a sequence (time dimension = width)\n",
    "        features = torch.mean(features, dim=2)  # (B, 128, 32)\n",
    "        features = features.permute(0, 2, 1)      # (B, 32, 128)\n",
    "        outputs, _ = self.lstm(features)         # (B, 32, 512)\n",
    "        outputs = self.fc(outputs)               # (B, 32, num_classes)\n",
    "        outputs = outputs.permute(1, 0, 2)         # (T, B, num_classes) for CTC loss\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 5. Define the End‑to‑End OCR Model for Multi‑Word Images\n",
    "# ------------------------------\n",
    "\n",
    "class EndToEndOCRMulti(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EndToEndOCRMulti, self).__init__()\n",
    "        # For this demo, we focus on recognition.\n",
    "        # In a full system, a multi‑object detection branch would also be included.\n",
    "        self.recognizer = TextRecognizer(num_classes)\n",
    "\n",
    "    def forward(self, images, gt_boxes_list):\n",
    "        \"\"\"\n",
    "        images: Tensor of shape (B, 3, H, W)\n",
    "        gt_boxes_list: List of length B; each element is a tensor of shape (N, 4) where N is the number\n",
    "                       of text boxes in that image (normalized to [0,1]).\n",
    "        Returns: Recognition logits of shape (T, total_boxes, num_classes)\n",
    "        \"\"\"\n",
    "        crops = []\n",
    "        batch_size = images.size(0)\n",
    "        for i in range(batch_size):\n",
    "            image = images[i:i+1]  # shape: (1, 3, H, W)\n",
    "            _, H, W = image.shape[1:]\n",
    "            # Loop over all bounding boxes for the current image.\n",
    "            for box in gt_boxes_list[i]:\n",
    "                # box = [x_min, y_min, x_max, y_max] in normalized coordinates.\n",
    "                x_min = int(box[0].item() * W)\n",
    "                y_min = int(box[1].item() * H)\n",
    "                x_max = int(box[2].item() * W)\n",
    "                y_max = int(box[3].item() * H)\n",
    "                x_min = max(x_min, 0)\n",
    "                y_min = max(y_min, 0)\n",
    "                x_max = min(x_max, W)\n",
    "                y_max = min(y_max, H)\n",
    "                # Crop the word region.\n",
    "                crop = image[:, :, y_min:y_max, x_min:x_max]\n",
    "                # Resize the crop to fixed size expected by the recognizer (e.g., 32x128).\n",
    "                crop = F.interpolate(crop, size=(32, 128), mode='bilinear', align_corners=False)\n",
    "                crops.append(crop)\n",
    "        if len(crops) == 0:\n",
    "            # In case no boxes are found, return an empty tensor.\n",
    "            return None\n",
    "        # Concatenate all cropped word regions into one batch.\n",
    "        crops = torch.cat(crops, dim=0)  # shape: (total_boxes, 3, 32, 128)\n",
    "        recog_logits = self.recognizer(crops)  # shape: (T, total_boxes, num_classes)\n",
    "        return recog_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Error processing file page_1_3_boxed.png: Unexpected type <class 'numpy.ndarray'>\n",
      "Error processing file page_1_4.png: Unexpected type <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     25\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mcontinue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Skip problematic batches\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 11\u001b[0m, in \u001b[0;36mocr_collate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mocr_collate_fn\u001b[39m(batch):\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    This collate function takes a list of samples from OCRMultiWordDataset.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    Each sample: (image, boxes, text_targets, target_lengths)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    It stacks the images into a batch tensor and keeps the others in lists.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     12\u001b[0m     images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(images, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m     boxes_list \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]          \u001b[38;5;66;03m# List of tensors (sizes vary per image)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define transforms – here just converting to tensor.\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((512, 256)),  # (height, width) - adjust as needed\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "    # Update these paths to point to your dataset directories.\n",
    "    images_dir = input_folder\n",
    "    annotations_dir = output_folder\n",
    "\n",
    "    dataset = OCRMultiWordDataset(images_dir, annotations_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=ocr_collate_fn, num_workers=0)  # Set num_workers=0 for debugging\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EndToEndOCRMulti(num_classes).to(device)\n",
    "\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 10\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            if batch is None:\n",
    "                continue  # Skip problematic batches\n",
    "            images, boxes_list, text_targets_list, target_lengths_list = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            recog_logits = model(images, boxes_list)\n",
    "            if recog_logits is None or recog_logits.size(1) == 0:\n",
    "                print(\"Skipping batch due to empty recog_logits\")\n",
    "                continue\n",
    "\n",
    "            T, total_words, _ = recog_logits.size()\n",
    "            input_lengths = torch.full(size=(total_words,), fill_value=T, dtype=torch.long).to(device)\n",
    "\n",
    "            targets = []\n",
    "            all_target_lengths = []\n",
    "            for text_list, length_list in zip(text_targets_list, target_lengths_list):\n",
    "                for seq, length in zip(text_list, length_list):\n",
    "                    targets.append(seq)\n",
    "                    all_target_lengths.append(length)\n",
    "\n",
    "            if len(targets) == 0:\n",
    "                print(\"Skipping batch due to empty targets\")\n",
    "                continue\n",
    "\n",
    "            targets_cat = torch.cat(targets).to(device)\n",
    "            target_lengths = torch.tensor(all_target_lengths, dtype=torch.long).to(device)\n",
    "\n",
    "            if targets_cat.size(0) != target_lengths.sum():\n",
    "                print(\"Mismatch between targets_cat and target_lengths. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            try:\n",
    "                loss_recog = ctc_loss_fn(recog_logits, targets_cat, input_lengths, target_lengths)\n",
    "                loss_recog.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss_recog.item()\n",
    "            except Exception as e:\n",
    "                print(f\"Error during loss computation: {e}\")\n",
    "                continue\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Recognition Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_text(seq):\n",
    "    # seq: list or numpy array of indices\n",
    "    # skips blanks (0) and out-of-range indices\n",
    "    return ''.join([charset[idx] for idx in seq if idx != 0 and idx < len(charset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00cd9460-a0de-4c24-b8b4-66115a729e7d.json\n",
      "Annotations: [{'text': 'මේ', 'bounding_box': [10, 12, 35, 32], 'confidence': 92}, {'text': 'අන්ඩුව', 'bounding_box': [50, 11, 106, 36], 'confidence': 91}, {'text': 'පත්\\u200c', 'bounding_box': [122, 11, 151, 32], 'confidence': 96}, {'text': 'කිරීම', 'bounding_box': [167, 12, 210, 32], 'confidence': 96}, {'text': 'ගෑන', 'bounding_box': [226, 18, 263, 35], 'confidence': 96}, {'text': 'මම', 'bounding_box': [279, 14, 307, 32], 'confidence': 95}, {'text': 'නම්\\u200c', 'bounding_box': [322, 12, 352, 32], 'confidence': 96}, {'text': 'දුක්\\u200c', 'bounding_box': [368, 11, 395, 36], 'confidence': 96}, {'text': 'වෙන්නේ', 'bounding_box': [411, 11, 479, 32], 'confidence': 96}, {'text': 'නෑ.', 'bounding_box': [10, 54, 31, 71], 'confidence': 96}, {'text': 'ඒකට', 'bounding_box': [51, 47, 95, 68], 'confidence': 96}, {'text': 'හේතුව', 'bounding_box': [110, 47, 166, 72], 'confidence': 94}, {'text': 'මෙන්න', 'bounding_box': [182, 47, 239, 68], 'confidence': 96}, {'text': 'මේකයි.', 'bounding_box': [255, 48, 310, 68], 'confidence': 96}, {'text': 'කාලෙකට', 'bounding_box': [330, 50, 407, 71], 'confidence': 96}, {'text': 'ඉස්සර', 'bounding_box': [423, 47, 476, 71], 'confidence': 96}, {'text': '80', 'bounding_box': [11, 88, 29, 104], 'confidence': 96}, {'text': 'දශකයේ', 'bounding_box': [45, 83, 112, 108], 'confidence': 96}, {'text': 'අග', 'bounding_box': [127, 90, 154, 107], 'confidence': 93}, {'text': 'භාගයෙදී', 'bounding_box': [170, 85, 242, 108], 'confidence': 92}, {'text': 'වාගේ', 'bounding_box': [257, 83, 303, 104], 'confidence': 93}, {'text': ';', 'bounding_box': [319, 100, 321, 106], 'confidence': 91}, {'text': 'ඊයෙ', 'bounding_box': [338, 87, 374, 104], 'confidence': 93}, {'text': 'පෙරෙදා', 'bounding_box': [389, 87, 450, 108], 'confidence': 92}, {'text': 'මියගිය', 'bounding_box': [11, 120, 67, 140], 'confidence': 96}, {'text': 'අජන්තා', 'bounding_box': [83, 119, 145, 143], 'confidence': 96}, {'text': 'රණසිංහ', 'bounding_box': [161, 121, 232, 140], 'confidence': 96}, {'text': 'ලොක්ක', 'bounding_box': [248, 119, 313, 143], 'confidence': 95}, {'text': 'තමයි', 'bounding_box': [328, 121, 372, 140], 'confidence': 93}, {'text': 'නවයුගය', 'bounding_box': [387, 122, 460, 144], 'confidence': 92}, {'text': 'පත්තරය', 'bounding_box': [10, 155, 81, 176], 'confidence': 93}, {'text': 'කලේ.', 'bounding_box': [97, 155, 140, 179], 'confidence': 88}, {'text': 'ඔය', 'bounding_box': [159, 158, 186, 176], 'confidence': 96}, {'text': 'කාලයේදීම', 'bounding_box': [202, 155, 286, 180], 'confidence': 95}, {'text': 'වගේ', 'bounding_box': [302, 155, 342, 176], 'confidence': 96}, {'text': 'නන්දා', 'bounding_box': [357, 155, 404, 180], 'confidence': 93}, {'text': 'මාලණී', 'bounding_box': [421, 156, 475, 179], 'confidence': 92}, {'text': '\"හේමන්ත්යෙදී\\'', 'bounding_box': [11, 191, 129, 216], 'confidence': 84}, {'text': 'අවසාන', 'bounding_box': [145, 194, 206, 215], 'confidence': 96}, {'text': 'කරලා', 'bounding_box': [221, 195, 270, 215], 'confidence': 91}, {'text': \"'සත්යයේ\", 'bounding_box': [286, 191, 360, 212], 'confidence': 40}, {'text': 'ගීතය\"', 'bounding_box': [375, 193, 426, 212], 'confidence': 89}, {'text': 'පටන්\\u200c', 'bounding_box': [442, 191, 486, 212], 'confidence': 96}, {'text': 'ගත්තේ.ඒ', 'bounding_box': [10, 227, 87, 248], 'confidence': 71}, {'text': 'කාලයේ', 'bounding_box': [102, 227, 163, 251], 'confidence': 96}, {'text': 'මේ', 'bounding_box': [178, 228, 203, 248], 'confidence': 93}, {'text': 'ප්රසන්ග', 'bounding_box': [218, 227, 289, 248], 'confidence': 89}, {'text': 'වල', 'bounding_box': [305, 230, 333, 251], 'confidence': 93}, {'text': 'ගැයුනු', 'bounding_box': [348, 234, 399, 252], 'confidence': 90}, {'text': 'ගීත', 'bounding_box': [414, 229, 446, 248], 'confidence': 96}, {'text': 'වලට', 'bounding_box': [461, 230, 502, 251], 'confidence': 95}, {'text': 'අණ්ඩුවේ', 'bounding_box': [10, 264, 81, 288], 'confidence': 82}, {'text': 'පත්තර', 'bounding_box': [97, 263, 155, 284], 'confidence': 96}, {'text': 'වලින්\\u200c', 'bounding_box': [170, 263, 215, 287], 'confidence': 96}, {'text': 'ඉස්ම', 'bounding_box': [230, 263, 271, 287], 'confidence': 93}, {'text': 'යන්න', 'bounding_box': [287, 263, 333, 284], 'confidence': 96}, {'text': 'ගැහුවා.', 'bounding_box': [348, 266, 404, 288], 'confidence': 96}, {'text': 'ඉතින්\\u200c', 'bounding_box': [424, 263, 470, 287], 'confidence': 96}, {'text': 'මටත්\\u200c', 'bounding_box': [11, 299, 54, 320], 'confidence': 96}, {'text': 'සෙට්\\u200c', 'bounding_box': [70, 300, 107, 320], 'confidence': 95}, {'text': 'උනා', 'bounding_box': [123, 306, 155, 323], 'confidence': 96}, {'text': 'එක', 'bounding_box': [171, 302, 202, 320], 'confidence': 97}, {'text': 'එක', 'bounding_box': [217, 302, 248, 320], 'confidence': 96}, {'text': 'කලා', 'bounding_box': [263, 306, 300, 323], 'confidence': 92}, {'text': 'බුවාලගෙන්\\u200c', 'bounding_box': [315, 299, 407, 324], 'confidence': 92}, {'text': 'ටෝක්ස්\\u200c', 'bounding_box': [422, 299, 483, 320], 'confidence': 95}, {'text': 'ටිකක්\\u200c', 'bounding_box': [10, 335, 58, 356], 'confidence': 93}, {'text': 'ගන්නදවසක්\\u200c', 'bounding_box': [74, 335, 181, 360], 'confidence': 89}, {'text': 'මටසෙට්\\u200c', 'bounding_box': [198, 336, 262, 356], 'confidence': 92}, {'text': 'උනා', 'bounding_box': [278, 342, 310, 359], 'confidence': 96}, {'text': 'අමරදේව', 'bounding_box': [326, 335, 398, 360], 'confidence': 93}, {'text': 'මාස්ටර්ව', 'bounding_box': [415, 335, 487, 356], 'confidence': 87}, {'text': 'හමුවෙන්න.', 'bounding_box': [10, 371, 98, 396], 'confidence': 94}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00ca7146-d38b-4b0a-b8fa-c97c99691338.json\n",
      "Annotations: [{'text': '1817-1818', 'bounding_box': [12, 16, 97, 32], 'confidence': 92}, {'text': 'ඌව', 'bounding_box': [113, 14, 147, 35], 'confidence': 95}, {'text': 'වෙල්ලස්ස', 'bounding_box': [163, 11, 243, 35], 'confidence': 94}, {'text': 'ජනතාව', 'bounding_box': [259, 14, 323, 32], 'confidence': 93}, {'text': 'බ්රිතාන්ය', 'bounding_box': [339, 11, 417, 32], 'confidence': 84}, {'text': 'රජයට', 'bounding_box': [432, 14, 484, 32], 'confidence': 96}, {'text': 'විරුද්ධව', 'bounding_box': [10, 47, 78, 72], 'confidence': 96}, {'text': 'නැගී', 'bounding_box': [94, 49, 130, 71], 'confidence': 96}, {'text': 'සිටීමෙන්\\u200c', 'bounding_box': [146, 47, 215, 68], 'confidence': 93}, {'text': 'බ්රිතාන්ය', 'bounding_box': [230, 47, 308, 68], 'confidence': 89}, {'text': 'හමුදා', 'bounding_box': [323, 50, 369, 72], 'confidence': 96}, {'text': 'විසින්\\u200c', 'bounding_box': [385, 47, 430, 68], 'confidence': 96}, {'text': 'එහි', 'bounding_box': [445, 49, 475, 68], 'confidence': 96}, {'text': 'වාසය', 'bounding_box': [10, 86, 57, 104], 'confidence': 88}, {'text': 'කළ', 'bounding_box': [72, 90, 102, 107], 'confidence': 96}, {'text': '10,000', 'bounding_box': [119, 88, 171, 106], 'confidence': 92}, {'text': 'පමණ', 'bounding_box': [187, 86, 235, 104], 'confidence': 96}, {'text': 'ජනතාවක්\\u200c', 'bounding_box': [251, 83, 333, 104], 'confidence': 96}, {'text': 'මරා', 'bounding_box': [350, 86, 380, 104], 'confidence': 96}, {'text': 'දමන', 'bounding_box': [396, 86, 436, 108], 'confidence': 96}, {'text': 'ලදහ.', 'bounding_box': [451, 90, 491, 108], 'confidence': 95}, {'text': 'සමහරු', 'bounding_box': [10, 122, 72, 143], 'confidence': 71}, {'text': 'ආගිය', 'bounding_box': [88, 121, 134, 143], 'confidence': 96}, {'text': 'අතක්\\u200c', 'bounding_box': [150, 119, 195, 143], 'confidence': 93}, {'text': 'නොමැතිවී', 'bounding_box': [211, 120, 292, 143], 'confidence': 92}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_000fb1af-e683-41a8-b2fb-3a2d5031b809.json\n",
      "Annotations: [{'text': 'කාටත්\\u200c', 'bounding_box': [10, 11, 62, 32], 'confidence': 94}, {'text': 'ළමා', 'bounding_box': [78, 14, 110, 35], 'confidence': 96}, {'text': 'සමයක්\\u200c', 'bounding_box': [126, 11, 185, 32], 'confidence': 96}, {'text': 'ඇත්තේ', 'bounding_box': [201, 11, 261, 35], 'confidence': 96}, {'text': 'ය.', 'bounding_box': [277, 18, 290, 32], 'confidence': 96}, {'text': 'ශ්රී', 'bounding_box': [310, 11, 338, 32], 'confidence': 96}, {'text': 'ලංකාවේ', 'bounding_box': [353, 12, 421, 35], 'confidence': 90}, {'text': 'බොහෝ', 'bounding_box': [437, 11, 501, 32], 'confidence': 96}, {'text': 'දෙනකුට', 'bounding_box': [10, 50, 77, 72], 'confidence': 95}, {'text': 'ගම්මානයක', 'bounding_box': [92, 48, 190, 68], 'confidence': 96}, {'text': 'ගත', 'bounding_box': [205, 54, 237, 68], 'confidence': 96}, {'text': 'වුණු', 'bounding_box': [252, 50, 286, 72], 'confidence': 96}, {'text': 'ළමා', 'bounding_box': [302, 50, 334, 71], 'confidence': 93}, {'text': 'වියක්\\u200c', 'bounding_box': [350, 47, 395, 68], 'confidence': 89}, {'text': 'අයත්\\u200c', 'bounding_box': [411, 47, 453, 71], 'confidence': 96}, {'text': 'ය.', 'bounding_box': [469, 54, 482, 68], 'confidence': 95}, {'text': 'එසේ', 'bounding_box': [10, 83, 48, 104], 'confidence': 96}, {'text': 'නම්\\u200c', 'bounding_box': [63, 84, 93, 104], 'confidence': 93}, {'text': 'වික්රමසිංහගේ', 'bounding_box': [109, 83, 232, 104], 'confidence': 83}, {'text': 'ළමා', 'bounding_box': [247, 86, 279, 107], 'confidence': 96}, {'text': 'සමය', 'bounding_box': [295, 86, 336, 104], 'confidence': 96}, {'text': 'සතු', 'bounding_box': [352, 90, 382, 108], 'confidence': 94}, {'text': 'සුවිශේෂී', 'bounding_box': [397, 83, 464, 108], 'confidence': 95}, {'text': 'ලක්ෂණය', 'bounding_box': [10, 119, 89, 143], 'confidence': 95}, {'text': 'කවරේද?', 'bounding_box': [105, 119, 175, 144], 'confidence': 96}, {'text': 'බොහෝ', 'bounding_box': [190, 119, 254, 140], 'confidence': 96}, {'text': 'දෙනාට', 'bounding_box': [269, 122, 324, 144], 'confidence': 96}, {'text': 'එසේ', 'bounding_box': [339, 119, 377, 140], 'confidence': 96}, {'text': 'සිතෙනු', 'bounding_box': [392, 121, 449, 144], 'confidence': 96}, {'text': 'ඇත.', 'bounding_box': [464, 126, 497, 143], 'confidence': 96}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00b44d86-7704-4eaa-8362-af4183e4e86c.json\n",
      "Annotations: [{'text': 'එදා', 'bounding_box': [10, 14, 39, 36], 'confidence': 96}, {'text': 'ඒ', 'bounding_box': [54, 11, 68, 32], 'confidence': 97}, {'text': 'කතාව', 'bounding_box': [83, 14, 134, 32], 'confidence': 96}, {'text': 'අවසන්\\u200c', 'bounding_box': [150, 11, 207, 35], 'confidence': 96}, {'text': 'වූයේ', 'bounding_box': [222, 11, 260, 36], 'confidence': 96}, {'text': 'ඔහුගේ', 'bounding_box': [276, 11, 333, 36], 'confidence': 96}, {'text': 'ඒ', 'bounding_box': [348, 11, 362, 32], 'confidence': 96}, {'text': 'කතාවත්\\u200c', 'bounding_box': [377, 11, 445, 32], 'confidence': 96}, {'text': 'සමගය.', 'bounding_box': [10, 50, 67, 68], 'confidence': 85}, {'text': 'කතාවේ', 'bounding_box': [87, 48, 149, 68], 'confidence': 96}, {'text': 'ඊලග', 'bounding_box': [165, 51, 207, 71], 'confidence': 96}, {'text': 'කොටස', 'bounding_box': [222, 50, 281, 68], 'confidence': 96}, {'text': 'පටන්\\u200c', 'bounding_box': [297, 47, 341, 68], 'confidence': 96}, {'text': 'ගන්නේ', 'bounding_box': [356, 47, 416, 68], 'confidence': 96}, {'text': 'ඒ', 'bounding_box': [431, 47, 445, 68], 'confidence': 96}, {'text': 'කතාවෙන්\\u200c', 'bounding_box': [10, 83, 90, 104], 'confidence': 96}, {'text': 'ටික', 'bounding_box': [105, 84, 135, 104], 'confidence': 93}, {'text': 'දවසකට', 'bounding_box': [151, 86, 219, 108], 'confidence': 96}, {'text': 'පසු', 'bounding_box': [234, 90, 260, 108], 'confidence': 96}, {'text': 'මා', 'bounding_box': [277, 86, 296, 104], 'confidence': 96}, {'text': 'රාජකාරී', 'bounding_box': [311, 84, 376, 104], 'confidence': 96}, {'text': 'කටයුත්තක්\\u200c', 'bounding_box': [391, 83, 485, 108], 'confidence': 96}, {'text': 'සදහා', 'bounding_box': [10, 126, 55, 144], 'confidence': 92}, {'text': 'සේවාදායකකු', 'bounding_box': [70, 119, 177, 144], 'confidence': 90}, {'text': 'හමුවීමට', 'bounding_box': [193, 120, 265, 144], 'confidence': 95}, {'text': 'ඔහුගේ', 'bounding_box': [282, 119, 339, 144], 'confidence': 96}, {'text': 'නිවසට', 'bounding_box': [354, 121, 411, 140], 'confidence': 96}, {'text': 'ගිය', 'bounding_box': [427, 121, 456, 140], 'confidence': 96}, {'text': \"'වේලාවකය.\", 'bounding_box': [10, 156, 99, 179], 'confidence': 82}, {'text': 'මා', 'bounding_box': [119, 158, 138, 176], 'confidence': 95}, {'text': 'එම', 'bounding_box': [153, 158, 181, 176], 'confidence': 96}, {'text': 'නිවෙසට', 'bounding_box': [196, 157, 264, 176], 'confidence': 96}, {'text': 'යන', 'bounding_box': [279, 162, 309, 176], 'confidence': 96}, {'text': 'විට', 'bounding_box': [324, 156, 351, 176], 'confidence': 96}, {'text': 'ඈත', 'bounding_box': [366, 162, 399, 179], 'confidence': 96}, {'text': 'තියාම', 'bounding_box': [414, 157, 463, 176], 'confidence': 96}, {'text': 'දුටුවේ.', 'bounding_box': [10, 192, 57, 216], 'confidence': 86}, {'text': 'ආලින්දය', 'bounding_box': [77, 191, 148, 216], 'confidence': 96}, {'text': 'පුරා', 'bounding_box': [163, 195, 193, 216], 'confidence': 96}, {'text': 'පුටු', 'bounding_box': [209, 194, 235, 216], 'confidence': 96}, {'text': 'කොට්ට', 'bounding_box': [251, 192, 310, 212], 'confidence': 93}, {'text': 'ගොඩක්\\u200c', 'bounding_box': [326, 191, 389, 212], 'confidence': 96}, {'text': 'ගසාගෙන', 'bounding_box': [405, 198, 482, 212], 'confidence': 96}, {'text': 'පුටුවක්\\u200c', 'bounding_box': [10, 227, 68, 252], 'confidence': 96}, {'text': 'උඩට', 'bounding_box': [84, 230, 123, 251], 'confidence': 96}, {'text': 'නැග', 'bounding_box': [139, 234, 175, 251], 'confidence': 96}, {'text': 'බිමට', 'bounding_box': [191, 228, 233, 248], 'confidence': 96}, {'text': 'පනින', 'bounding_box': [249, 229, 295, 248], 'confidence': 96}, {'text': 'අවුරුදු', 'bounding_box': [310, 230, 362, 252], 'confidence': 96}, {'text': '3ක්\\u200c', 'bounding_box': [379, 227, 405, 248], 'confidence': 96}, {'text': 'පමන', 'bounding_box': [421, 230, 465, 248], 'confidence': 97}, {'text': 'වූ', 'bounding_box': [480, 230, 493, 252], 'confidence': 96}, {'text': 'පනින', 'bounding_box': [10, 265, 56, 284], 'confidence': 95}, {'text': 'කුඩා', 'bounding_box': [71, 266, 108, 288], 'confidence': 95}, {'text': 'කොලු', 'bounding_box': [123, 270, 170, 288], 'confidence': 93}, {'text': 'ගැටයෙකි.', 'bounding_box': [185, 265, 260, 287], 'confidence': 92}, {'text': 'නිවස', 'bounding_box': [280, 265, 323, 284], 'confidence': 96}, {'text': 'ආසන්නයට', 'bounding_box': [339, 263, 430, 287], 'confidence': 96}, {'text': 'වෙත්ම', 'bounding_box': [445, 263, 500, 284], 'confidence': 93}, {'text': 'ඔහු', 'bounding_box': [11, 302, 41, 324], 'confidence': 96}, {'text': 'උඩ', 'bounding_box': [57, 302, 83, 323], 'confidence': 96}, {'text': 'සිට', 'bounding_box': [98, 301, 125, 320], 'confidence': 96}, {'text': 'පනින්නේ', 'bounding_box': [140, 299, 214, 320], 'confidence': 96}, {'text': 'පුලුන්\\u200c', 'bounding_box': [229, 299, 274, 324], 'confidence': 96}, {'text': 'පිරවූ', 'bounding_box': [289, 301, 328, 324], 'confidence': 93}, {'text': 'ටෙඩියෙකු', 'bounding_box': [344, 300, 424, 324], 'confidence': 92}, {'text': 'මතට', 'bounding_box': [440, 302, 483, 320], 'confidence': 96}, {'text': 'බව', 'bounding_box': [10, 338, 38, 356], 'confidence': 93}, {'text': 'හෙදින්ම', 'bounding_box': [54, 335, 126, 360], 'confidence': 91}, {'text': 'මා', 'bounding_box': [143, 338, 162, 356], 'confidence': 96}, {'text': 'දුටුවෙමි.', 'bounding_box': [177, 336, 238, 360], 'confidence': 96}, {'text': 'මේ', 'bounding_box': [258, 336, 283, 356], 'confidence': 96}, {'text': 'අනිවාර්යෙන්ම', 'bounding_box': [298, 335, 412, 359], 'confidence': 96}, {'text': 'පොඩි', 'bounding_box': [428, 336, 471, 356], 'confidence': 96}, {'text': 'තේන්සිනා', 'bounding_box': [10, 371, 90, 392], 'confidence': 84}, {'text': 'කෙනෙක්\\u200c', 'bounding_box': [106, 371, 177, 392], 'confidence': 96}, {'text': 'බව', 'bounding_box': [193, 374, 221, 392], 'confidence': 96}, {'text': 'මම', 'bounding_box': [238, 374, 266, 392], 'confidence': 96}, {'text': 'ඒ', 'bounding_box': [274, 365, 287, 403], 'confidence': 96}, {'text': 'ඉරියව්\\u200c', 'bounding_box': [310, 372, 362, 395], 'confidence': 96}, {'text': 'වලින්\\u200c', 'bounding_box': [378, 371, 423, 395], 'confidence': 96}, {'text': 'දුටුවෙමි.', 'bounding_box': [438, 372, 499, 396], 'confidence': 95}, {'text': 'අවුරුදු', 'bounding_box': [10, 410, 62, 432], 'confidence': 96}, {'text': '3ක්\\u200c', 'bounding_box': [79, 407, 105, 428], 'confidence': 96}, {'text': 'වත්\\u200c', 'bounding_box': [121, 407, 151, 428], 'confidence': 96}, {'text': 'නොවූ', 'bounding_box': [167, 410, 212, 432], 'confidence': 96}, {'text': 'කොලු', 'bounding_box': [228, 414, 275, 432], 'confidence': 96}, {'text': 'ගැටයා', 'bounding_box': [290, 410, 342, 431], 'confidence': 93}, {'text': 'ටෙඩියා', 'bounding_box': [358, 408, 415, 428], 'confidence': 91}, {'text': 'කුඩුපට්ටම', 'bounding_box': [10, 444, 96, 468], 'confidence': 74}, {'text': 'කර', 'bounding_box': [112, 447, 141, 464], 'confidence': 93}, {'text': 'දැමීමෙ', 'bounding_box': [156, 444, 210, 468], 'confidence': 90}, {'text': 'මහා', 'bounding_box': [227, 446, 262, 464], 'confidence': 93}, {'text': 'මෙහෙයුමකය', 'bounding_box': [277, 446, 387, 468], 'confidence': 92}, {'text': 'විටින්\\u200c', 'bounding_box': [403, 443, 447, 464], 'confidence': 96}, {'text': 'විට', 'bounding_box': [462, 444, 489, 464], 'confidence': 96}, {'text': 'රූපවාහිනි', 'bounding_box': [10, 481, 93, 503], 'confidence': 93}, {'text': 'රෙස්ලින්\\u200c', 'bounding_box': [108, 479, 176, 503], 'confidence': 94}, {'text': 'තරගවලින්\\u200c', 'bounding_box': [191, 479, 280, 503], 'confidence': 93}, {'text': 'අහුලාගත්තා', 'bounding_box': [295, 479, 396, 504], 'confidence': 88}, {'text': 'යැයි', 'bounding_box': [412, 481, 443, 503], 'confidence': 96}, {'text': 'සිතිය', 'bounding_box': [459, 481, 502, 500], 'confidence': 96}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00edf6db-a5e1-408c-b651-d9f7ec838fde.json\n",
      "Annotations: [{'text': 'මගේ', 'bounding_box': [11, 11, 51, 32], 'confidence': 96}, {'text': 'ලිස්ට්\\u200c', 'bounding_box': [66, 11, 107, 35], 'confidence': 95}, {'text': 'එක්\\u200c', 'bounding_box': [123, 11, 164, 32], 'confidence': 96}, {'text': 'නම්\\u200c', 'bounding_box': [180, 12, 210, 32], 'confidence': 96}, {'text': 'ඔතන', 'bounding_box': [227, 14, 273, 32], 'confidence': 95}, {'text': 'තව', 'bounding_box': [288, 14, 317, 32], 'confidence': 95}, {'text': 'එකක්\\u200c', 'bounding_box': [333, 11, 381, 32], 'confidence': 96}, {'text': 'තියාගෙන', 'bounding_box': [397, 13, 474, 32], 'confidence': 96}, {'text': 'ඉන්නේ.', 'bounding_box': [10, 47, 67, 71], 'confidence': 93}, {'text': 'ප්රේමය්\\u200c', 'bounding_box': [85, 47, 150, 68], 'confidence': 91}, {'text': 'බලාපොරොත්තුව', 'bounding_box': [165, 47, 302, 72], 'confidence': 93}, {'text': 'සංතුෂ්ඨිය', 'bounding_box': [318, 47, 397, 72], 'confidence': 76}, {'text': 'කියලා', 'bounding_box': [413, 49, 463, 71], 'confidence': 96}, {'text': 'මොන', 'bounding_box': [10, 86, 57, 104], 'confidence': 96}, {'text': 'අයිසිං', 'bounding_box': [72, 85, 120, 107], 'confidence': 96}, {'text': 'දැම්මත්\\u200c', 'bounding_box': [135, 83, 195, 108], 'confidence': 92}, {'text': 'ඕක', 'bounding_box': [212, 84, 243, 104], 'confidence': 96}, {'text': 'ඇතුලෙ', 'bounding_box': [258, 90, 316, 108], 'confidence': 96}, {'text': 'තියෙන්නෙ', 'bounding_box': [331, 83, 414, 104], 'confidence': 95}, {'text': 'අපි', 'bounding_box': [429, 85, 454, 107], 'confidence': 96}, {'text': 'හැමදාම', 'bounding_box': [10, 122, 75, 144], 'confidence': 93}, {'text': 'රහවිදින', 'bounding_box': [91, 120, 161, 144], 'confidence': 66}, {'text': 'ඒ', 'bounding_box': [176, 119, 190, 140], 'confidence': 96}, {'text': 'කේක්\\u200c', 'bounding_box': [205, 119, 250, 140], 'confidence': 93}, {'text': 'කෑල්ලමයි.', 'bounding_box': [266, 119, 345, 143], 'confidence': 86}]\n",
      "Loaded JSON file: D:\\python\\data\\boxed\\image_00f17190-69bd-459e-8e27-96a3b85c1d6f.json\n",
      "Annotations: [{'text': 'කාලෙන්\\u200c', 'bounding_box': [10, 11, 74, 35], 'confidence': 96}, {'text': 'කාලෙට', 'bounding_box': [89, 14, 149, 35], 'confidence': 93}, {'text': 'හිටිද', 'bounding_box': [164, 12, 204, 36], 'confidence': 44}, {'text': 'එහෙම', 'bounding_box': [219, 14, 274, 32], 'confidence': 96}, {'text': 'නැද්ද', 'bounding_box': [289, 11, 329, 36], 'confidence': 96}, {'text': 'කියන්න', 'bounding_box': [345, 11, 408, 32], 'confidence': 96}, {'text': 'මම', 'bounding_box': [424, 14, 452, 32], 'confidence': 96}, {'text': 'දන්නෙ', 'bounding_box': [10, 47, 62, 72], 'confidence': 96}, {'text': 'නෑ.', 'bounding_box': [78, 54, 99, 71], 'confidence': 96}, {'text': 'එදිනෙදා', 'bounding_box': [119, 49, 184, 72], 'confidence': 96}, {'text': 'යන', 'bounding_box': [200, 54, 230, 68], 'confidence': 96}, {'text': 'එන', 'bounding_box': [245, 50, 275, 68], 'confidence': 97}, {'text': 'හැම', 'bounding_box': [290, 50, 326, 71], 'confidence': 97}, {'text': 'තැනකදීම', 'bounding_box': [341, 49, 419, 72], 'confidence': 96}, {'text': 'හමුවෙන', 'bounding_box': [10, 86, 81, 108], 'confidence': 74}, {'text': 'අය', 'bounding_box': [97, 90, 122, 107], 'confidence': 97}, {'text': 'මට', 'bounding_box': [139, 86, 166, 104], 'confidence': 96}, {'text': 'දක්වන', 'bounding_box': [181, 83, 238, 108], 'confidence': 89}, {'text': 'ආදරය', 'bounding_box': [253, 87, 305, 108], 'confidence': 96}, {'text': 'කරුණාව', 'bounding_box': [321, 86, 394, 107], 'confidence': 96}, {'text': '6ළෙන්ගතුකම', 'bounding_box': [10, 119, 113, 144], 'confidence': 53}, {'text': 'නිසා', 'bounding_box': [128, 121, 163, 140], 'confidence': 96}, {'text': 'ඔවුන්\\u200c', 'bounding_box': [179, 119, 224, 144], 'confidence': 96}, {'text': 'අතර', 'bounding_box': [239, 123, 279, 143], 'confidence': 96}, {'text': 'ආදරණීය', 'bounding_box': [294, 120, 367, 144], 'confidence': 96}, {'text': 'චරිතයක්\\u200c', 'bounding_box': [383, 119, 456, 140], 'confidence': 96}, {'text': 'කියලනම්\\u200c', 'bounding_box': [10, 156, 86, 179], 'confidence': 96}, {'text': 'කියන්න', 'bounding_box': [101, 155, 164, 176], 'confidence': 96}, {'text': 'පුළුවන්\\u200c.', 'bounding_box': [179, 155, 239, 180], 'confidence': 96}]\n",
      "Word-level accuracy: 0.4754 (135/284)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4753521126760563"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_decode(logits):\n",
    "    # logits: (T, N, C) where T = time, N = batch*words, C = num_classes\n",
    "    # Returns: list of lists of predicted indices (one per word)\n",
    "    probs = logits.softmax(2)\n",
    "    max_probs, max_indices = probs.max(2)  # (T, N)\n",
    "    max_indices = max_indices.cpu().numpy().T  # (N, T)\n",
    "    decoded = []\n",
    "    for seq in max_indices:\n",
    "        # Collapse repeats and remove blanks (assume blank=0)\n",
    "        prev = -1\n",
    "        out = []\n",
    "        for idx in seq:\n",
    "            if idx != prev and idx != 0:\n",
    "                out.append(idx)\n",
    "            prev = idx\n",
    "        decoded.append(out)\n",
    "    return decoded\n",
    "\n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "            images, boxes_list, text_targets_list, target_lengths_list = batch\n",
    "            images = images.to(device)\n",
    "            recog_logits = model(images, boxes_list)\n",
    "            if recog_logits is None or recog_logits.size(1) == 0:\n",
    "                continue\n",
    "            decoded = greedy_decode(recog_logits)\n",
    "            # Flatten ground truth targets\n",
    "            gt_texts = []\n",
    "            for text_list in text_targets_list:\n",
    "                for seq in text_list:\n",
    "                    gt_texts.append(sequence_to_text(seq.cpu().numpy()))\n",
    "            # Convert predictions to text\n",
    "            pred_texts = [sequence_to_text(seq) for seq in decoded]\n",
    "            # Compare predictions and ground truth\n",
    "            for gt, pred in zip(gt_texts, pred_texts):\n",
    "                if gt == pred:\n",
    "                    correct_words += 1\n",
    "                total_words += 1\n",
    "    acc = correct_words / total_words if total_words > 0 else 0\n",
    "    print(f\"Word-level accuracy: {acc:.4f} ({correct_words}/{total_words})\")\n",
    "    return acc\n",
    "\n",
    "# Example usage after training:\n",
    "compute_accuracy(model, dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
