{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Pre-processing pages â†’ line crops â€¦\n",
      "Unique stoi clusters: {'\"': 1, '%': 2, ',': 3, '.': 4, '0': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '7': 11, '8': 12, '9': 13, 'à¶…': 14, 'à¶†': 15, 'à¶‡': 16, 'à¶‰': 17, 'à¶‹': 18, 'à¶‘': 19, 'à¶’': 20, 'à¶”': 21, 'à¶š': 22, 'à¶šà·Š': 23, 'à¶šà·Š\\u200c': 24, 'à¶šà·': 25, 'à¶šà·’': 26, 'à¶šà·“': 27, 'à¶šà·”': 28, 'à¶šà·™': 29, 'à¶šà·œ': 30, 'à¶œ': 31, 'à¶œà·Š': 32, 'à¶œà·': 33, 'à¶œà·’': 34, 'à¶œà·“': 35, 'à¶œà·”': 36, 'à¶œà·™': 37, 'à¶œà·š': 38, 'à¶œà·œ': 39, 'à¶œà·': 40, 'à¶Ÿ': 41, 'à¶¢': 42, 'à¶¢à·Š': 43, 'à¶¢à·’': 44, 'à¶¢à·”': 45, 'à¶§': 46, 'à¶§à·Š\\u200c': 47, 'à¶§à·’': 48, 'à¶§à·“': 49, 'à¶§à·': 50, 'à¶¨': 51, 'à¶©': 52, 'à¶©à·Š': 53, 'à¶©à·': 54, 'à¶«': 55, 'à¶«à·': 56, 'à¶«à·’': 57, 'à¶«à·”': 58, 'à¶«à·š': 59, 'à¶¬': 60, 'à¶­': 61, 'à¶­à·Š': 62, 'à¶­à·': 63, 'à¶­à·': 64, 'à¶­à·’': 65, 'à¶­à·“': 66, 'à¶­à·”': 67, 'à¶­à·”à¶‚': 68, 'à¶­à·š': 69, 'à¶¯': 70, 'à¶¯à·Š': 71, 'à¶¯à·': 72, 'à¶¯à·’': 73, 'à¶¯à·“': 74, 'à¶¯à·”': 75, 'à¶¯à·™': 76, 'à¶¯à·š': 77, 'à¶°': 78, 'à¶°à·': 79, 'à¶±': 80, 'à¶±à¶‚': 81, 'à¶±à·Š': 82, 'à¶±à·': 83, 'à¶±à·': 84, 'à¶±à·‘': 85, 'à¶±à·’': 86, 'à¶±à·“': 87, 'à¶±à·”': 88, 'à¶±à·™': 89, 'à¶±à·š': 90, 'à¶±à·œ': 91, 'à¶³': 92, 'à¶³à·”': 93, 'à¶´': 94, 'à¶´à·Š': 95, 'à¶´à·': 96, 'à¶´à·': 97, 'à¶´à·‘': 98, 'à¶´à·’': 99, 'à¶´à·”': 100, 'à¶´à·˜': 101, 'à¶´à·™': 102, 'à¶´à·œ': 103, 'à¶¶': 104, 'à¶¶à·Š': 105, 'à¶¶à·': 106, 'à¶¶à·': 107, 'à¶¶à·‘': 108, 'à¶¶à·“': 109, 'à¶¶à·–': 110, 'à¶¶à·™': 111, 'à¶¶à·š': 112, 'à¶·à¶‚': 113, 'à¶·à·': 114, 'à¶¸': 115, 'à¶¸à¶‚': 116, 'à¶¸à·Š': 117, 'à¶¸à·': 118, 'à¶¸à·à¶‚': 119, 'à¶¸à·': 120, 'à¶¸à·’': 121, 'à¶¸à·“': 122, 'à¶¸à·”': 123, 'à¶¸à·™': 124, 'à¶¸à·š': 125, 'à¶º': 126, 'à¶ºà·': 127, 'à¶ºà·‘': 128, 'à¶ºà·’': 129, 'à¶ºà·“': 130, 'à¶ºà·”': 131, 'à¶ºà·™': 132, 'à¶ºà·š': 133, 'à¶ºà·œ': 134, 'à¶ºà·': 135, 'à¶»': 136, 'à¶»à¶‚': 137, 'à¶»à·Š': 138, 'à¶»à·': 139, 'à¶»à·': 140, 'à¶»à·‘': 141, 'à¶»à·’': 142, 'à¶»à·“': 143, 'à¶»à·”': 144, 'à¶»à·™': 145, 'à¶»à·œ': 146, 'à¶»à·': 147, 'à¶½': 148, 'à¶½à¶‚': 149, 'à¶½à·Š': 150, 'à¶½à·': 151, 'à¶½à·': 152, 'à¶½à·’': 153, 'à¶½à·”': 154, 'à¶½à·™': 155, 'à¶½à·': 156, 'à·€': 157, 'à·€à¶‚': 158, 'à·€à·': 159, 'à·€à·': 160, 'à·€à·’': 161, 'à·€à·“': 162, 'à·€à·”': 163, 'à·€à·–': 164, 'à·€à·™': 165, 'à·€à·š': 166, 'à·': 167, 'à·à·Š': 168, 'à·à·’': 169, 'à·‚': 170, 'à·‚à·Š': 171, 'à·‚à·’': 172, 'à·ƒ': 173, 'à·ƒà·Š': 174, 'à·ƒà·Š\\u200c': 175, 'à·ƒà·': 176, 'à·ƒà·': 177, 'à·ƒà·’': 178, 'à·ƒà·”': 179, 'à·ƒà·š': 180, 'à·ƒà·œ': 181, 'à·ƒà·': 182, 'à·„': 183, 'à·„à·': 184, 'à·„à·': 185, 'à·„à·’': 186, 'à·„à·”': 187, 'à·„à·™': 188, 'à·„à·œ': 189, 'à·„à·': 190, 'à·…': 191, 'à·…à·': 192, 'à·…à·’': 193, 'à·…à·”': 194, 'â€˜': 195, 'â€™': 196, ' ': 197}\n",
      "Unique grapheme clusters: ['\"', '%', ',', '.', '0', '1', '2', '3', '4', '5', '7', '8', '9', 'à¶…', 'à¶†', 'à¶‡', 'à¶‰', 'à¶‹', 'à¶‘', 'à¶’', 'à¶”', 'à¶š', 'à¶šà·Š', 'à¶šà·Š\\u200c', 'à¶šà·', 'à¶šà·’', 'à¶šà·“', 'à¶šà·”', 'à¶šà·™', 'à¶šà·œ', 'à¶œ', 'à¶œà·Š', 'à¶œà·', 'à¶œà·’', 'à¶œà·“', 'à¶œà·”', 'à¶œà·™', 'à¶œà·š', 'à¶œà·œ', 'à¶œà·', 'à¶Ÿ', 'à¶¢', 'à¶¢à·Š', 'à¶¢à·’', 'à¶¢à·”', 'à¶§', 'à¶§à·Š\\u200c', 'à¶§à·’', 'à¶§à·“', 'à¶§à·', 'à¶¨', 'à¶©', 'à¶©à·Š', 'à¶©à·', 'à¶«', 'à¶«à·', 'à¶«à·’', 'à¶«à·”', 'à¶«à·š', 'à¶¬', 'à¶­', 'à¶­à·Š', 'à¶­à·', 'à¶­à·', 'à¶­à·’', 'à¶­à·“', 'à¶­à·”', 'à¶­à·”à¶‚', 'à¶­à·š', 'à¶¯', 'à¶¯à·Š', 'à¶¯à·', 'à¶¯à·’', 'à¶¯à·“', 'à¶¯à·”', 'à¶¯à·™', 'à¶¯à·š', 'à¶°', 'à¶°à·', 'à¶±', 'à¶±à¶‚', 'à¶±à·Š', 'à¶±à·', 'à¶±à·', 'à¶±à·‘', 'à¶±à·’', 'à¶±à·“', 'à¶±à·”', 'à¶±à·™', 'à¶±à·š', 'à¶±à·œ', 'à¶³', 'à¶³à·”', 'à¶´', 'à¶´à·Š', 'à¶´à·', 'à¶´à·', 'à¶´à·‘', 'à¶´à·’', 'à¶´à·”', 'à¶´à·˜', 'à¶´à·™', 'à¶´à·œ', 'à¶¶', 'à¶¶à·Š', 'à¶¶à·', 'à¶¶à·', 'à¶¶à·‘', 'à¶¶à·“', 'à¶¶à·–', 'à¶¶à·™', 'à¶¶à·š', 'à¶·à¶‚', 'à¶·à·', 'à¶¸', 'à¶¸à¶‚', 'à¶¸à·Š', 'à¶¸à·', 'à¶¸à·à¶‚', 'à¶¸à·', 'à¶¸à·’', 'à¶¸à·“', 'à¶¸à·”', 'à¶¸à·™', 'à¶¸à·š', 'à¶º', 'à¶ºà·', 'à¶ºà·‘', 'à¶ºà·’', 'à¶ºà·“', 'à¶ºà·”', 'à¶ºà·™', 'à¶ºà·š', 'à¶ºà·œ', 'à¶ºà·', 'à¶»', 'à¶»à¶‚', 'à¶»à·Š', 'à¶»à·', 'à¶»à·', 'à¶»à·‘', 'à¶»à·’', 'à¶»à·“', 'à¶»à·”', 'à¶»à·™', 'à¶»à·œ', 'à¶»à·', 'à¶½', 'à¶½à¶‚', 'à¶½à·Š', 'à¶½à·', 'à¶½à·', 'à¶½à·’', 'à¶½à·”', 'à¶½à·™', 'à¶½à·', 'à·€', 'à·€à¶‚', 'à·€à·', 'à·€à·', 'à·€à·’', 'à·€à·“', 'à·€à·”', 'à·€à·–', 'à·€à·™', 'à·€à·š', 'à·', 'à·à·Š', 'à·à·’', 'à·‚', 'à·‚à·Š', 'à·‚à·’', 'à·ƒ', 'à·ƒà·Š', 'à·ƒà·Š\\u200c', 'à·ƒà·', 'à·ƒà·', 'à·ƒà·’', 'à·ƒà·”', 'à·ƒà·š', 'à·ƒà·œ', 'à·ƒà·', 'à·„', 'à·„à·', 'à·„à·', 'à·„à·’', 'à·„à·”', 'à·„à·™', 'à·„à·œ', 'à·„à·', 'à·…', 'à·…à·', 'à·…à·’', 'à·…à·”', 'â€˜', 'â€™', ' ']\n",
      "Total unique clusters: 197\n",
      "âœ… Saved 53 line crops.\n",
      "Avg lines per page: 7.571428571428571\n",
      "labels.json written with 53 entries â†’ D:\\python\\data\\line_crops\\labels.json\n",
      "Batch 0:\n",
      "  Images shape: torch.Size([8, 1, 32, 512])\n",
      "  Labels shape: torch.Size([237])\n",
      "  Target lengths: [28, 30, 31, 30, 32, 28, 30, 28]\n",
      "  Ground truth texts: ['à¶‘à·„à·™à¶­à·Š à¶´à·™à¶¶à¶»à·€à·à¶»à·’ 17 à·€à·à¶±à·’ à¶¯à· à¶½à·’à¶´à·’à¶ºà¶§ à¶…à¶±à·”à·€ à¶’', 'à¶…à¶±à·”à·€ à¶¯à·™à·€à·’à¶ºà¶±à·Š à·€à·„à¶±à·Šà·ƒà·š à·€à·’à·à·Šà·€à¶º à¶¸à·€à· à¶‡à¶­à·Šà¶­à·š à¶¸à·’à¶±à·’à·ƒà·à¶œà·š', 'à¶šà¶»à¶± à¶¶à·€à¶§ à¶´à·Šà¶»à¶šà·à·à¶ºà¶šà·Š\\u200c à¶šà¶»à¶± à¶½à·™à·ƒà¶§ à·„à·’à¶§à·Š\\u200cà¶½à¶»à·Š à¶”à·„à·”à¶§', 'à¶šà¶½à·à¶´à¶ºà·š à¶»à¶§à·€à¶½à¶§ à¶’ à·ƒà¶³à·„à· à¶œà·”à·€à¶±à·Š à¶´à·Šà¶»à·€à·šà·à¶´à¶­à·Šà¶» à·„à·', 'à¶¶à¶§à·„à·’à¶» à¶¸à¶­à·€à·à¶¯à¶ºà¶šà·’. à¶…à¶¸à¶»à¶­à·”à¶‚à¶œ à¶¸à·„à¶­à·à¶œà·š à¶…à·€à·”à¶½à·Š à¶…à¶¯à·„à·ƒà·Š', 'à¶¸à·™à·„à·™à¶¯à·’ à¶šà¶½à· à¶šà¶§à¶ºà·”à¶­à·”à·€à¶½à¶§ à¶ºà·œà¶¸à·” à·€à·”à¶«à·š à¶±à·à¶­à·’ à¶­à¶»à¶¸à·Š.', 'à¶ºà·”à¶­à·Šà¶­à·š. à¶´à¶±à·Šà·ƒà·’à¶½à·Š à¶´à¶¯ à¶´à·„à¶¸ à¶»à·à¶šà·Šà¶šà·œà¶­à·Š à·„à·œà¶³à¶ºà·’. à¶¶à·à¶ºà·’à¶±à¶‚', 'à·ƒà·’à¶§à·“à¶¸à¶§ à¶‰à¶©à¶šà·Š à¶±à·œà¶¯à·“ à¶¸à·’à¶­à·Šà¶» à¶´à·à¶šà·Š\\u200cà·‚à·’à¶šà¶ºà¶±à·Šà¶§ à¶·à·à¶»à¶¯à·“à¶¸à·š']\n",
      "  Sample 0 label indices: [19, 188, 62, 197, 102, 104, 136, 159, 142, 197, 6, 11, 197, 160, 86, 197, 72, 197, 153, 99, 126, 46, 197, 14, 88, 157, 197, 20]\n",
      "  Sample 1 label indices: [14, 88, 157, 197, 76, 161, 126, 82, 197, 157, 183, 82, 180, 197, 161, 168, 157, 126, 197, 115, 159, 197, 16, 62, 69, 197, 121, 86, 176, 38]\n",
      "  Sample 2 label indices: [22, 136, 80, 197, 104, 157, 46, 197, 95, 136, 25, 167, 126, 24, 197, 22, 136, 80, 197, 155, 173, 46, 197, 186, 47, 148, 138, 197, 21, 187, 46]\n",
      "  Sample 3 label indices: [22, 151, 94, 133, 197, 136, 46, 157, 148, 46, 197, 20, 197, 173, 92, 184, 197, 36, 157, 82, 197, 95, 136, 166, 167, 94, 62, 136, 197, 184]\n",
      "  Sample 4 label indices: [104, 46, 186, 136, 197, 115, 61, 159, 70, 126, 26, 4, 197, 14, 115, 136, 68, 31, 197, 115, 183, 63, 38, 197, 14, 163, 150, 197, 14, 70, 183, 174]\n",
      "  Sample 5 label indices: [124, 188, 73, 197, 22, 151, 197, 22, 46, 131, 67, 157, 148, 46, 197, 134, 123, 197, 163, 59, 197, 84, 65, 197, 61, 136, 117, 4]\n",
      "  Sample 6 label indices: [131, 62, 69, 4, 197, 94, 82, 178, 150, 197, 94, 70, 197, 94, 183, 115, 197, 140, 23, 30, 62, 197, 189, 92, 129, 4, 197, 107, 129, 81]\n",
      "  Sample 7 label indices: [178, 49, 115, 46, 197, 17, 52, 23, 197, 91, 74, 197, 121, 62, 136, 197, 96, 24, 172, 22, 126, 82, 46, 197, 114, 136, 74, 125]\n",
      "Dataset size 53 â†’ train 43, val 10\n",
      "Epoch 01/10 âŸ¶ train loss 97.297 | CER 344.29% | W-Acc 0.0%   ||   val loss 77.217 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 02/10 âŸ¶ train loss 75.148 | CER 99.47% | W-Acc 0.0%   ||   val loss 52.504 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 03/10 âŸ¶ train loss 41.471 | CER 100.00% | W-Acc 0.0%   ||   val loss 22.343 | CER 100.00% | W-Acc 0.0%\n",
      "ğŸŸ¢ ResNet encoder unfrozen\n",
      "Epoch 04/10 âŸ¶ train loss 14.438 | CER 100.00% | W-Acc 0.0%   ||   val loss 6.821 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 05/10 âŸ¶ train loss 5.784 | CER 100.00% | W-Acc 0.0%   ||   val loss 5.068 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 06/10 âŸ¶ train loss 5.074 | CER 100.00% | W-Acc 0.0%   ||   val loss 5.091 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 07/10 âŸ¶ train loss 5.061 | CER 100.00% | W-Acc 0.0%   ||   val loss 5.117 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 08/10 âŸ¶ train loss 5.022 | CER 100.00% | W-Acc 0.0%   ||   val loss 5.037 | CER 100.00% | W-Acc 0.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 368\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mparameters(): p\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸŸ¢ ResNet encoder unfrozen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 368\u001b[0m tr_loss, tr_cer, tr_wa \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m vl_loss, vl_cer, vl_wa \u001b[38;5;241m=\u001b[39m run_epoch(val_loader,   train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m âŸ¶ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | CER \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_cer\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | W-Acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_wa\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%   ||   \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvl_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | CER \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvl_cer\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | W-Acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvl_wa\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[55], line 348\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, train)\u001b[0m\n\u001b[0;32m    346\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels, inp_lens, tgt_lens)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m--> 348\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(); \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m; optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    349\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m B\n\u001b[0;32m    351\u001b[0m preds \u001b[38;5;241m=\u001b[39m greedy_decode(logits)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt           # optional for sanity-check\n",
    "import regex  # pip install regex  (needed for \\X = Unicode grapheme)\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "from matplotlib import font_manager\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.  EDIT THESE PATHS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PAGE_DIR = Path(r\"D:\\python\\data\\images\")          # page-level images\n",
    "ANN_FILE = Path(r\"D:\\python\\data\\json\\labels.json\")\n",
    "CROP_DIR = Path(r\"D:\\python\\data\\line_crops\")      # new crops + ckpts\n",
    "CKPT_DIR = CROP_DIR / \"checkpoints\"\n",
    "\n",
    "font_path = \"C:/Users/ASUS/Downloads/Noto_Sans_Sinhala,Yuji_Mai/Noto_Sans_Sinhala/NotoSansSinhala-VariableFont_wdth,wght.ttf\"  # update this path\n",
    "# Register the font\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "plt.rcParams['font.family'] = 'Noto Sans Sinhala'\n",
    "\n",
    "# training params\n",
    "IMG_H, IMG_W = 32, 512\n",
    "BATCH_SIZE   = 8\n",
    "EPOCHS       = 10\n",
    "LR           = 1e-4\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create dirs\n",
    "CROP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1.  BUILD LINE CROPS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â³ Pre-processing pages â†’ line crops â€¦\")\n",
    "\n",
    "with open(ANN_FILE, encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "label_map   = {}                # crop_name â†’ GT string\n",
    "line_counts = defaultdict(int)\n",
    "\n",
    "pad_vert = 4                    # vertical padding (px) â€“ adjust if needed\n",
    "pad_horz = 4                     # horizontal padding (px)\n",
    "\n",
    "cluster_re = regex.compile(r\"\\X\", flags=regex.UNICODE)\n",
    "all_clusters: set[str] = set()\n",
    "\n",
    "def get_clusters(s: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns extended grapheme clusters (base + diacritics + virama).\n",
    "    Filters out whitespace-only clusters.\n",
    "    \"\"\"\n",
    "    return [c for c in cluster_re.findall(s) if not c.isspace()]\n",
    "\n",
    "for entry in raw:\n",
    "    for uid, words in entry.items():\n",
    "        page_path = PAGE_DIR / f\"image_{uid}.png\"\n",
    "        if not page_path.exists():\n",
    "            # print(\"âš ï¸  missing image:\", page_path)\n",
    "            continue\n",
    "\n",
    "        # â”€â”€ group words into lines by their y-centres â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        lines = []\n",
    "        for w in sorted(words, key=lambda w: w[\"y\"]):\n",
    "            cy = w[\"y\"]\n",
    "            placed = False\n",
    "            for line in lines:\n",
    "                if abs(cy - line[\"cy\"]) < 15:       # 15-px threshold\n",
    "                    line[\"words\"].append(w)\n",
    "                    line[\"cy\"] = (line[\"cy\"] + cy) / 2\n",
    "                    placed = True\n",
    "                    break\n",
    "            if not placed:\n",
    "                lines.append({\"cy\": cy, \"words\": [w]})\n",
    "\n",
    "        # â”€â”€ crop each line robustly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        page = Image.open(page_path).convert(\"L\")\n",
    "        W_page, H_page = page.size\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            bxs, bys, bxe, bye = [], [], [], []\n",
    "            for w in line[\"words\"]:\n",
    "                x  = w[\"x\"]\n",
    "                y  = w[\"y\"]\n",
    "                w_ = w.get(\"w\", w.get(\"width\",  0))\n",
    "                h_ = w.get(\"h\", w.get(\"height\", 0))\n",
    "                # some JSONs store x1/y1 directly\n",
    "                x1 = w.get(\"x1\", x + w_)\n",
    "                y1 = w.get(\"y1\", y + h_)\n",
    "\n",
    "                bxs.append(x);      bys.append(y)\n",
    "                bxe.append(x1);     bye.append(y1)\n",
    "\n",
    "            # fallback: if every y1 == y (heights missing)\n",
    "            if max(bye) == min(bys):\n",
    "                est_line_h = int(H_page * 0.05)     # â‰ˆ 5 % page height\n",
    "                bye = [y + est_line_h for y in bys]\n",
    "\n",
    "            x0 = max(min(bxs) - pad_horz, 0)\n",
    "            y0 = max(min(bys) - pad_vert, 0)\n",
    "            x1 = min(max(bxe) + pad_horz, W_page)\n",
    "            y1 = min(max(bye) + pad_vert, H_page)\n",
    "\n",
    "            crop = page.crop((x0, y0, x1, y1))\n",
    "            crop_name = f\"image_{uid}_line{idx:02d}.png\"\n",
    "            crop.save(CROP_DIR / crop_name)\n",
    "\n",
    "            sorted_words = sorted(line[\"words\"], key=lambda w: w[\"x\"])\n",
    "            text = \" \".join(w[\"text\"] for w in sorted_words).strip()\n",
    "            label_map[crop_name] = text\n",
    "            line_counts[uid] += 1\n",
    "                \n",
    "        for ann in words:\n",
    "            txt = ann.get(\"text\", \"\")\n",
    "            all_clusters.update(get_clusters(txt))\n",
    "            \n",
    "charset = sorted(all_clusters)\n",
    "\n",
    "# Ensure the space character is included in the charset\n",
    "if ' ' not in charset:\n",
    "    charset.append(' ')\n",
    "\n",
    "stoi     = {c: i + 1 for i, c in enumerate(charset)}   # 0 = blank\n",
    "itos     = {i: c for c, i in stoi.items()}\n",
    "\n",
    "print(\"Unique stoi clusters:\", stoi)\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Unique grapheme clusters:\", charset)\n",
    "print(\"Total unique clusters:\", len(charset))\n",
    "print(f\"âœ… Saved {len(label_map)} line crops.\")\n",
    "print(\"Avg lines per page:\", sum(line_counts.values()) / len(line_counts))\n",
    "\n",
    "vocab_path = CROP_DIR / \"chars.txt\"\n",
    "with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for cluster in charset:\n",
    "        f.write(cluster + \"\\n\")\n",
    "\n",
    "# ---------- dump to disk ----------\n",
    "out_path = CROP_DIR / \"labels.json\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"labels.json written with {len(label_map)} entries â†’ {out_path}\")\n",
    "\n",
    "def resize_and_pad(img: Image.Image,\n",
    "                   target_size=(32, 512),\n",
    "                   fill=255) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Keeps aspect ratio, resizes by height, then right-pads (or bottom-pads)\n",
    "    with the `fill` colour (white = 255 for â€œLâ€ mode).\n",
    "    \"\"\"\n",
    "    tgt_h, tgt_w = target_size\n",
    "    w, h = img.size\n",
    "    scale = tgt_h / h\n",
    "    new_w = int(w * scale)\n",
    "    img_rs = img.resize((new_w, tgt_h), Image.BILINEAR)\n",
    "\n",
    "    # if the resized width exceeds target â†’ centre-crop, else pad\n",
    "    if new_w >= tgt_w:\n",
    "        img_rs = img_rs.crop((0, 0, tgt_w, tgt_h))\n",
    "        return img_rs\n",
    "    else:\n",
    "        pad_w = tgt_w - new_w\n",
    "        padding = (0, 0, pad_w, 0)              # (left, top, right, bottom)\n",
    "        return ImageOps.expand(img_rs, padding, fill=fill)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3.  DATASET & LOADER  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class LineOCRDataset(Dataset):\n",
    "    def __init__(self, crop_dir, label_map, stoi, transform):\n",
    "        self.paths = sorted([crop_dir/p for p in os.listdir(crop_dir)\n",
    "                             if p.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
    "        self.label_map, self.stoi, self.transform = label_map, stoi, transform\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(f\"Processing image {idx+1}/{len(self)}: {self.paths[idx].name}\")  \n",
    "        img  = Image.open(self.paths[idx]).convert(\"L\")\n",
    "           # Show the original image before any transform\n",
    "        # for i in range(idx <5): \n",
    "        #     plt.imshow(img, cmap=\"gray\")\n",
    "        #     # plt.title(f\"Original Image: '{self.label_map[self.paths[idx].name]}'\")\n",
    "        #     plt.axis(\"off\")\n",
    "        #     plt.show()\n",
    "   \n",
    "        if self.transform: \n",
    "            img  = resize_and_pad(img, target_size=(IMG_H, IMG_W))\n",
    "            \n",
    "            img = self.transform(img)\n",
    "        text = self.label_map[self.paths[idx].name]\n",
    "        clusters = regex.findall(r\"\\X\", text)\n",
    "        label = torch.tensor([self.stoi[c] for c in clusters if c in self.stoi],\n",
    "                             dtype=torch.long)\n",
    "                # img_t is a tensor of shape (1, IMG_H, IMG_W)\n",
    "        # # Undo normalization if needed (your normalization is mean=0.5, std=0.5)\n",
    "        # img_vis = img * 0.5 + 0.5  # unnormalize to [0,1]\n",
    "\n",
    "        # # Convert to PIL image for display\n",
    "        # img_pil = F.to_pil_image(img_vis)\n",
    "\n",
    "        # for i in range(idx <5):  # or any condition\n",
    "        #     plt.imshow(img_pil, cmap=\"gray\")\n",
    "        #     # plt.title(f\"Transformed Image: '{text}'\")\n",
    "        #     plt.axis(\"off\")\n",
    "        #     plt.show()\n",
    "\n",
    "        return img, label, text           # keep raw text for metrics\n",
    "\n",
    "def collate(batch):\n",
    "    imgs, labels, gts = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    tgt_lens = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    labels = torch.cat(labels)\n",
    "    return imgs, labels, tgt_lens, list(gts)\n",
    "\n",
    "transforms = T.Compose([\n",
    "    T.Resize((IMG_H, IMG_W)),           # fixed width â‡’ fixed T\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# transforms identical â€¦\n",
    "dataset = LineOCRDataset(CROP_DIR, label_map, stoi, transforms)\n",
    "\n",
    "# reproducible 80 / 20 split\n",
    "g = torch.Generator().manual_seed(42)\n",
    "n_val   = max(1, int(0.2 * len(dataset)))\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [len(dataset)-n_val, n_val], generator=g)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate, num_workers=0)\n",
    "\n",
    "# Check the first batch from the train_loader\n",
    "for batch_idx, (imgs, labels, tgt_lens, gts) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  Images shape: {imgs.shape}\")      # (batch_size, 1, IMG_H, IMG_W)\n",
    "    print(f\"  Labels shape: {labels.shape}\")    # (sum of label lengths,)\n",
    "    print(f\"  Target lengths: {tgt_lens.tolist()}\")  # List of label lengths\n",
    "    print(f\"  Ground truth texts: {gts}\")       # List of raw text strings\n",
    "\n",
    "    # Optionally, decode the first label in the batch\n",
    "    start = 0\n",
    "    for i, l in enumerate(tgt_lens):\n",
    "        label_indices = labels[start:start+l]\n",
    "        print(f\"  Sample {i} label indices: {label_indices.tolist()}\")\n",
    "        start += l\n",
    "    break  # Only check the first batch\n",
    "\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate, num_workers=0)\n",
    "\n",
    "print(f\"Dataset size {len(dataset)} â†’ train {len(train_ds)}, val {len(val_ds)}\")\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4.  MODEL  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, n_chars):\n",
    "        super().__init__()\n",
    "        base = models.resnet18()\n",
    "\n",
    "        # keep full width resolution\n",
    "        base.conv1.stride = (1, 1)\n",
    "        base.maxpool = nn.Identity()\n",
    "        for layer in [base.layer2, base.layer3, base.layer4]:\n",
    "            for m in layer:\n",
    "                if hasattr(m, \"conv1\") and m.conv1.stride == (2, 2):\n",
    "                    m.conv1.stride = (2, 1)\n",
    "                    if m.downsample:\n",
    "                        m.downsample[0].stride = (2, 1)\n",
    "\n",
    "        self.encoder = nn.Sequential(*list(base.children())[:-2])  # [B,512,H',W']\n",
    "        feat_h = IMG_H // 8                                        # height â†“ 4\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=512 * feat_h,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(512, n_chars + 1)  # + blank\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(1, 3, 1, 1)            # gray â†’ 3-chan5\n",
    "        feat = self.encoder(x)              # [B,512,H',W']\n",
    "        B, C, H, W = feat.size()\n",
    "        seq = feat.permute(0, 3, 1, 2).contiguous().view(B, W, C * H)\n",
    "        rnn_out, _ = self.rnn(seq)          # [B,W,512]\n",
    "        logits = self.classifier(rnn_out)   # [B,W,n+1]\n",
    "        return logits.log_softmax(-1).permute(1, 0, 2)  # [T,B,C]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5.  TRAINING  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dataset = LineOCRDataset(CROP_DIR, label_map, stoi, transforms)\n",
    "loader  = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                     shuffle=True, collate_fn=collate, num_workers=0)\n",
    "\n",
    "model     = CRNN(len(stoi)).to(DEVICE)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "FREEZE_EPOCHS = 3               # freeze ResNet for the first N epochs\n",
    "for p in model.encoder.parameters(): p.requires_grad = False\n",
    "\n",
    "blank_idx = 0\n",
    "itos_full = {0:\"\"} | itos       # map 0â†’\"\" for easy decoding\n",
    "\n",
    "def greedy_decode(log_probs):                     # [T,B,C]\n",
    "    best = log_probs.argmax(-1).cpu().T           # [B,T]\n",
    "    sentences = []\n",
    "    for seq in best:\n",
    "        dedup = [k for k,_ in itertools.groupby(seq.tolist()) if k != blank_idx]\n",
    "        sentences.append(\"\".join(itos_full[i] for i in dedup))\n",
    "    return sentences\n",
    "\n",
    "def cer(ref, hyp):\n",
    "    import numpy as np\n",
    "    m, n = len(ref), len(hyp)\n",
    "    dp = np.zeros((m+1, n+1), int)\n",
    "    dp[:,0] = range(m+1); dp[0] = range(n+1)\n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            dp[i,j] = min(dp[i-1,j]+1, dp[i,j-1]+1,\n",
    "                          dp[i-1,j-1] + (ref[i-1]!=hyp[j-1]))\n",
    "    return dp[m,n] / max(1,m)\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "    total_loss, chars, cer_sum, word_hit, total_words = 0, 0, 0, 0, 0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for imgs, labels, tgt_lens, gts in loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            logits = model(imgs)                # [T,B,C]\n",
    "            Tt, B, _ = logits.shape\n",
    "            inp_lens = torch.full((B,), Tt, dtype=torch.long, device=DEVICE)\n",
    "            loss = criterion(logits, labels, inp_lens, tgt_lens)\n",
    "            if train:\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            total_loss += loss.item() * B\n",
    "\n",
    "            preds = greedy_decode(logits)\n",
    "            for gt, pr in zip(gts, preds):\n",
    "                cer_sum    += cer(gt, pr) * len(gt)\n",
    "                chars      += len(gt)\n",
    "                word_hit   += (gt == pr)\n",
    "                total_words += 1\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    cer_rate = cer_sum / chars\n",
    "    word_acc = 100 * word_hit / total_words\n",
    "    return avg_loss, cer_rate, word_acc\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6.  TRAIN LOOP  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    if ep == FREEZE_EPOCHS+1:                 # unfreeze backbone\n",
    "        for p in model.encoder.parameters(): p.requires_grad = True\n",
    "        print(\"ğŸŸ¢ ResNet encoder unfrozen\")\n",
    "\n",
    "    tr_loss, tr_cer, tr_wa = run_epoch(train_loader, train=True)\n",
    "    vl_loss, vl_cer, vl_wa = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(f\"Epoch {ep:02d}/{EPOCHS} âŸ¶ \"\n",
    "          f\"train loss {tr_loss:.3f} | CER {tr_cer:.2%} | W-Acc {tr_wa:.1f}%   ||   \"\n",
    "          f\"val loss {vl_loss:.3f} | CER {vl_cer:.2%} | W-Acc {vl_wa:.1f}%\")\n",
    "\n",
    "    if ep % 5 == 0:\n",
    "        torch.save(model.state_dict(), CKPT_DIR / f\"crnn_ep{ep}.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), CKPT_DIR / \"crnn_final.pth\")\n",
    "print(\"ğŸ‰ Done â€“ model + metrics saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Pre-processing pages â†’ line crops â€¦\n",
      "Unique stoi clusters: {'\"': 1, '%': 2, ',': 3, '.': 4, '0': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '7': 11, '8': 12, '9': 13, 'à¶…': 14, 'à¶†': 15, 'à¶‡': 16, 'à¶‰': 17, 'à¶‹': 18, 'à¶‘': 19, 'à¶’': 20, 'à¶”': 21, 'à¶š': 22, 'à¶šà·Š': 23, 'à¶šà·Š\\u200c': 24, 'à¶šà·': 25, 'à¶šà·’': 26, 'à¶šà·“': 27, 'à¶šà·”': 28, 'à¶šà·™': 29, 'à¶šà·œ': 30, 'à¶œ': 31, 'à¶œà·Š': 32, 'à¶œà·': 33, 'à¶œà·’': 34, 'à¶œà·“': 35, 'à¶œà·”': 36, 'à¶œà·™': 37, 'à¶œà·š': 38, 'à¶œà·œ': 39, 'à¶œà·': 40, 'à¶Ÿ': 41, 'à¶¢': 42, 'à¶¢à·Š': 43, 'à¶¢à·’': 44, 'à¶¢à·”': 45, 'à¶§': 46, 'à¶§à·Š\\u200c': 47, 'à¶§à·’': 48, 'à¶§à·“': 49, 'à¶§à·': 50, 'à¶¨': 51, 'à¶©': 52, 'à¶©à·Š': 53, 'à¶©à·': 54, 'à¶«': 55, 'à¶«à·': 56, 'à¶«à·’': 57, 'à¶«à·”': 58, 'à¶«à·š': 59, 'à¶¬': 60, 'à¶­': 61, 'à¶­à·Š': 62, 'à¶­à·': 63, 'à¶­à·': 64, 'à¶­à·’': 65, 'à¶­à·“': 66, 'à¶­à·”': 67, 'à¶­à·”à¶‚': 68, 'à¶­à·š': 69, 'à¶¯': 70, 'à¶¯à·Š': 71, 'à¶¯à·': 72, 'à¶¯à·’': 73, 'à¶¯à·“': 74, 'à¶¯à·”': 75, 'à¶¯à·™': 76, 'à¶¯à·š': 77, 'à¶°': 78, 'à¶°à·': 79, 'à¶±': 80, 'à¶±à¶‚': 81, 'à¶±à·Š': 82, 'à¶±à·': 83, 'à¶±à·': 84, 'à¶±à·‘': 85, 'à¶±à·’': 86, 'à¶±à·“': 87, 'à¶±à·”': 88, 'à¶±à·™': 89, 'à¶±à·š': 90, 'à¶±à·œ': 91, 'à¶³': 92, 'à¶³à·”': 93, 'à¶´': 94, 'à¶´à·Š': 95, 'à¶´à·': 96, 'à¶´à·': 97, 'à¶´à·‘': 98, 'à¶´à·’': 99, 'à¶´à·”': 100, 'à¶´à·˜': 101, 'à¶´à·™': 102, 'à¶´à·œ': 103, 'à¶¶': 104, 'à¶¶à·Š': 105, 'à¶¶à·': 106, 'à¶¶à·': 107, 'à¶¶à·‘': 108, 'à¶¶à·“': 109, 'à¶¶à·–': 110, 'à¶¶à·™': 111, 'à¶¶à·š': 112, 'à¶·à¶‚': 113, 'à¶·à·': 114, 'à¶¸': 115, 'à¶¸à¶‚': 116, 'à¶¸à·Š': 117, 'à¶¸à·': 118, 'à¶¸à·à¶‚': 119, 'à¶¸à·': 120, 'à¶¸à·’': 121, 'à¶¸à·“': 122, 'à¶¸à·”': 123, 'à¶¸à·™': 124, 'à¶¸à·š': 125, 'à¶º': 126, 'à¶ºà·': 127, 'à¶ºà·‘': 128, 'à¶ºà·’': 129, 'à¶ºà·“': 130, 'à¶ºà·”': 131, 'à¶ºà·™': 132, 'à¶ºà·š': 133, 'à¶ºà·œ': 134, 'à¶ºà·': 135, 'à¶»': 136, 'à¶»à¶‚': 137, 'à¶»à·Š': 138, 'à¶»à·': 139, 'à¶»à·': 140, 'à¶»à·‘': 141, 'à¶»à·’': 142, 'à¶»à·“': 143, 'à¶»à·”': 144, 'à¶»à·™': 145, 'à¶»à·œ': 146, 'à¶»à·': 147, 'à¶½': 148, 'à¶½à¶‚': 149, 'à¶½à·Š': 150, 'à¶½à·': 151, 'à¶½à·': 152, 'à¶½à·’': 153, 'à¶½à·”': 154, 'à¶½à·™': 155, 'à¶½à·': 156, 'à·€': 157, 'à·€à¶‚': 158, 'à·€à·': 159, 'à·€à·': 160, 'à·€à·’': 161, 'à·€à·“': 162, 'à·€à·”': 163, 'à·€à·–': 164, 'à·€à·™': 165, 'à·€à·š': 166, 'à·': 167, 'à·à·Š': 168, 'à·à·’': 169, 'à·‚': 170, 'à·‚à·Š': 171, 'à·‚à·’': 172, 'à·ƒ': 173, 'à·ƒà·Š': 174, 'à·ƒà·Š\\u200c': 175, 'à·ƒà·': 176, 'à·ƒà·': 177, 'à·ƒà·’': 178, 'à·ƒà·”': 179, 'à·ƒà·š': 180, 'à·ƒà·œ': 181, 'à·ƒà·': 182, 'à·„': 183, 'à·„à·': 184, 'à·„à·': 185, 'à·„à·’': 186, 'à·„à·”': 187, 'à·„à·™': 188, 'à·„à·œ': 189, 'à·„à·': 190, 'à·…': 191, 'à·…à·': 192, 'à·…à·’': 193, 'à·…à·”': 194, 'â€˜': 195, 'â€™': 196, ' ': 197}\n",
      "Unique grapheme clusters: ['\"', '%', ',', '.', '0', '1', '2', '3', '4', '5', '7', '8', '9', 'à¶…', 'à¶†', 'à¶‡', 'à¶‰', 'à¶‹', 'à¶‘', 'à¶’', 'à¶”', 'à¶š', 'à¶šà·Š', 'à¶šà·Š\\u200c', 'à¶šà·', 'à¶šà·’', 'à¶šà·“', 'à¶šà·”', 'à¶šà·™', 'à¶šà·œ', 'à¶œ', 'à¶œà·Š', 'à¶œà·', 'à¶œà·’', 'à¶œà·“', 'à¶œà·”', 'à¶œà·™', 'à¶œà·š', 'à¶œà·œ', 'à¶œà·', 'à¶Ÿ', 'à¶¢', 'à¶¢à·Š', 'à¶¢à·’', 'à¶¢à·”', 'à¶§', 'à¶§à·Š\\u200c', 'à¶§à·’', 'à¶§à·“', 'à¶§à·', 'à¶¨', 'à¶©', 'à¶©à·Š', 'à¶©à·', 'à¶«', 'à¶«à·', 'à¶«à·’', 'à¶«à·”', 'à¶«à·š', 'à¶¬', 'à¶­', 'à¶­à·Š', 'à¶­à·', 'à¶­à·', 'à¶­à·’', 'à¶­à·“', 'à¶­à·”', 'à¶­à·”à¶‚', 'à¶­à·š', 'à¶¯', 'à¶¯à·Š', 'à¶¯à·', 'à¶¯à·’', 'à¶¯à·“', 'à¶¯à·”', 'à¶¯à·™', 'à¶¯à·š', 'à¶°', 'à¶°à·', 'à¶±', 'à¶±à¶‚', 'à¶±à·Š', 'à¶±à·', 'à¶±à·', 'à¶±à·‘', 'à¶±à·’', 'à¶±à·“', 'à¶±à·”', 'à¶±à·™', 'à¶±à·š', 'à¶±à·œ', 'à¶³', 'à¶³à·”', 'à¶´', 'à¶´à·Š', 'à¶´à·', 'à¶´à·', 'à¶´à·‘', 'à¶´à·’', 'à¶´à·”', 'à¶´à·˜', 'à¶´à·™', 'à¶´à·œ', 'à¶¶', 'à¶¶à·Š', 'à¶¶à·', 'à¶¶à·', 'à¶¶à·‘', 'à¶¶à·“', 'à¶¶à·–', 'à¶¶à·™', 'à¶¶à·š', 'à¶·à¶‚', 'à¶·à·', 'à¶¸', 'à¶¸à¶‚', 'à¶¸à·Š', 'à¶¸à·', 'à¶¸à·à¶‚', 'à¶¸à·', 'à¶¸à·’', 'à¶¸à·“', 'à¶¸à·”', 'à¶¸à·™', 'à¶¸à·š', 'à¶º', 'à¶ºà·', 'à¶ºà·‘', 'à¶ºà·’', 'à¶ºà·“', 'à¶ºà·”', 'à¶ºà·™', 'à¶ºà·š', 'à¶ºà·œ', 'à¶ºà·', 'à¶»', 'à¶»à¶‚', 'à¶»à·Š', 'à¶»à·', 'à¶»à·', 'à¶»à·‘', 'à¶»à·’', 'à¶»à·“', 'à¶»à·”', 'à¶»à·™', 'à¶»à·œ', 'à¶»à·', 'à¶½', 'à¶½à¶‚', 'à¶½à·Š', 'à¶½à·', 'à¶½à·', 'à¶½à·’', 'à¶½à·”', 'à¶½à·™', 'à¶½à·', 'à·€', 'à·€à¶‚', 'à·€à·', 'à·€à·', 'à·€à·’', 'à·€à·“', 'à·€à·”', 'à·€à·–', 'à·€à·™', 'à·€à·š', 'à·', 'à·à·Š', 'à·à·’', 'à·‚', 'à·‚à·Š', 'à·‚à·’', 'à·ƒ', 'à·ƒà·Š', 'à·ƒà·Š\\u200c', 'à·ƒà·', 'à·ƒà·', 'à·ƒà·’', 'à·ƒà·”', 'à·ƒà·š', 'à·ƒà·œ', 'à·ƒà·', 'à·„', 'à·„à·', 'à·„à·', 'à·„à·’', 'à·„à·”', 'à·„à·™', 'à·„à·œ', 'à·„à·', 'à·…', 'à·…à·', 'à·…à·’', 'à·…à·”', 'â€˜', 'â€™', ' ']\n",
      "Total unique clusters: 197\n",
      "âœ… Saved 53 line crops.\n",
      "Avg lines per page: 7.571428571428571\n",
      "labels.json written with 53 entries â†’ D:\\python\\data\\line_crops\\labels.json\n",
      "Batch 0:\n",
      "  Images shape: torch.Size([8, 1, 32, 512])\n",
      "  Labels shape: torch.Size([237])\n",
      "  Target lengths: [28, 30, 31, 30, 32, 28, 30, 28]\n",
      "  Ground truth texts: ['à¶‘à·„à·™à¶­à·Š à¶´à·™à¶¶à¶»à·€à·à¶»à·’ 17 à·€à·à¶±à·’ à¶¯à· à¶½à·’à¶´à·’à¶ºà¶§ à¶…à¶±à·”à·€ à¶’', 'à¶…à¶±à·”à·€ à¶¯à·™à·€à·’à¶ºà¶±à·Š à·€à·„à¶±à·Šà·ƒà·š à·€à·’à·à·Šà·€à¶º à¶¸à·€à· à¶‡à¶­à·Šà¶­à·š à¶¸à·’à¶±à·’à·ƒà·à¶œà·š', 'à¶šà¶»à¶± à¶¶à·€à¶§ à¶´à·Šà¶»à¶šà·à·à¶ºà¶šà·Š\\u200c à¶šà¶»à¶± à¶½à·™à·ƒà¶§ à·„à·’à¶§à·Š\\u200cà¶½à¶»à·Š à¶”à·„à·”à¶§', 'à¶šà¶½à·à¶´à¶ºà·š à¶»à¶§à·€à¶½à¶§ à¶’ à·ƒà¶³à·„à· à¶œà·”à·€à¶±à·Š à¶´à·Šà¶»à·€à·šà·à¶´à¶­à·Šà¶» à·„à·', 'à¶¶à¶§à·„à·’à¶» à¶¸à¶­à·€à·à¶¯à¶ºà¶šà·’. à¶…à¶¸à¶»à¶­à·”à¶‚à¶œ à¶¸à·„à¶­à·à¶œà·š à¶…à·€à·”à¶½à·Š à¶…à¶¯à·„à·ƒà·Š', 'à¶¸à·™à·„à·™à¶¯à·’ à¶šà¶½à· à¶šà¶§à¶ºà·”à¶­à·”à·€à¶½à¶§ à¶ºà·œà¶¸à·” à·€à·”à¶«à·š à¶±à·à¶­à·’ à¶­à¶»à¶¸à·Š.', 'à¶ºà·”à¶­à·Šà¶­à·š. à¶´à¶±à·Šà·ƒà·’à¶½à·Š à¶´à¶¯ à¶´à·„à¶¸ à¶»à·à¶šà·Šà¶šà·œà¶­à·Š à·„à·œà¶³à¶ºà·’. à¶¶à·à¶ºà·’à¶±à¶‚', 'à·ƒà·’à¶§à·“à¶¸à¶§ à¶‰à¶©à¶šà·Š à¶±à·œà¶¯à·“ à¶¸à·’à¶­à·Šà¶» à¶´à·à¶šà·Š\\u200cà·‚à·’à¶šà¶ºà¶±à·Šà¶§ à¶·à·à¶»à¶¯à·“à¶¸à·š']\n",
      "  Sample 0 label indices: [19, 188, 62, 197, 102, 104, 136, 159, 142, 197, 6, 11, 197, 160, 86, 197, 72, 197, 153, 99, 126, 46, 197, 14, 88, 157, 197, 20]\n",
      "  Sample 1 label indices: [14, 88, 157, 197, 76, 161, 126, 82, 197, 157, 183, 82, 180, 197, 161, 168, 157, 126, 197, 115, 159, 197, 16, 62, 69, 197, 121, 86, 176, 38]\n",
      "  Sample 2 label indices: [22, 136, 80, 197, 104, 157, 46, 197, 95, 136, 25, 167, 126, 24, 197, 22, 136, 80, 197, 155, 173, 46, 197, 186, 47, 148, 138, 197, 21, 187, 46]\n",
      "  Sample 3 label indices: [22, 151, 94, 133, 197, 136, 46, 157, 148, 46, 197, 20, 197, 173, 92, 184, 197, 36, 157, 82, 197, 95, 136, 166, 167, 94, 62, 136, 197, 184]\n",
      "  Sample 4 label indices: [104, 46, 186, 136, 197, 115, 61, 159, 70, 126, 26, 4, 197, 14, 115, 136, 68, 31, 197, 115, 183, 63, 38, 197, 14, 163, 150, 197, 14, 70, 183, 174]\n",
      "  Sample 5 label indices: [124, 188, 73, 197, 22, 151, 197, 22, 46, 131, 67, 157, 148, 46, 197, 134, 123, 197, 163, 59, 197, 84, 65, 197, 61, 136, 117, 4]\n",
      "  Sample 6 label indices: [131, 62, 69, 4, 197, 94, 82, 178, 150, 197, 94, 70, 197, 94, 183, 115, 197, 140, 23, 30, 62, 197, 189, 92, 129, 4, 197, 107, 129, 81]\n",
      "  Sample 7 label indices: [178, 49, 115, 46, 197, 17, 52, 23, 197, 91, 74, 197, 121, 62, 136, 197, 96, 24, 172, 22, 126, 82, 46, 197, 114, 136, 74, 125]\n",
      "Dataset size 53 â†’ train 43, val 10\n",
      "Epoch 01/10 âŸ¶ train loss 97.297 | CER 344.29% | W-Acc 0.0%   ||   val loss 77.217 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 02/10 âŸ¶ train loss 75.148 | CER 99.47% | W-Acc 0.0%   ||   val loss 52.504 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 03/10 âŸ¶ train loss 41.471 | CER 100.00% | W-Acc 0.0%   ||   val loss 22.343 | CER 100.00% | W-Acc 0.0%\n",
      "ğŸŸ¢ ResNet encoder unfrozen\n",
      "Epoch 04/10 âŸ¶ train loss 14.438 | CER 100.00% | W-Acc 0.0%   ||   val loss 6.821 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 05/10 âŸ¶ train loss 5.784 | CER 100.00% | W-Acc 0.0%   ||   val loss 5.068 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 06/10 âŸ¶ train loss 5.074 | CER 100.00% | W-Acc 0.0%   ||   val loss 5.091 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 07/10 âŸ¶ train loss 5.061 | CER 100.00% | W-Acc 0.0%   ||   val loss 5.117 | CER 100.00% | W-Acc 0.0%\n",
      "Epoch 08/10 âŸ¶ train loss 5.022 | CER 100.00% | W-Acc 0.0%   ||   val loss 5.037 | CER 100.00% | W-Acc 0.0%\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt           # optional for sanity-check\n",
    "import regex  # pip install regex  (needed for \\X = Unicode grapheme)\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "from matplotlib import font_manager\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.  EDIT THESE PATHS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PAGE_DIR = Path(r\"D:\\python\\data\\images\")          # page-level images\n",
    "ANN_FILE = Path(r\"D:\\python\\data\\json\\labels.json\")\n",
    "CROP_DIR = Path(r\"D:\\python\\data\\line_crops\")      # new crops + ckpts\n",
    "CKPT_DIR = CROP_DIR / \"checkpoints\"\n",
    "\n",
    "font_path = \"C:/Users/ASUS/Downloads/Noto_Sans_Sinhala,Yuji_Mai/Noto_Sans_Sinhala/NotoSansSinhala-VariableFont_wdth,wght.ttf\"  # update this path\n",
    "# Register the font\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "plt.rcParams['font.family'] = 'Noto Sans Sinhala'\n",
    "\n",
    "# training params\n",
    "IMG_H, IMG_W = 32, 512\n",
    "BATCH_SIZE   = 8\n",
    "EPOCHS       = 10\n",
    "LR           = 1e-4\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create dirs\n",
    "CROP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1.  BUILD LINE CROPS  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"â³ Pre-processing pages â†’ line crops â€¦\")\n",
    "\n",
    "with open(ANN_FILE, encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "label_map   = {}                # crop_name â†’ GT string\n",
    "line_counts = defaultdict(int)\n",
    "\n",
    "pad_vert = 4                    # vertical padding (px) â€“ adjust if needed\n",
    "pad_horz = 4                     # horizontal padding (px)\n",
    "\n",
    "cluster_re = regex.compile(r\"\\X\", flags=regex.UNICODE)\n",
    "all_clusters: set[str] = set()\n",
    "\n",
    "def get_clusters(s: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns extended grapheme clusters (base + diacritics + virama).\n",
    "    Filters out whitespace-only clusters.\n",
    "    \"\"\"\n",
    "    return [c for c in cluster_re.findall(s) if not c.isspace()]\n",
    "\n",
    "for entry in raw:\n",
    "    for uid, words in entry.items():\n",
    "        page_path = PAGE_DIR / f\"image_{uid}.png\"\n",
    "        if not page_path.exists():\n",
    "            # print(\"âš ï¸  missing image:\", page_path)\n",
    "            continue\n",
    "\n",
    "        # â”€â”€ group words into lines by their y-centres â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        lines = []\n",
    "        for w in sorted(words, key=lambda w: w[\"y\"]):\n",
    "            cy = w[\"y\"]\n",
    "            placed = False\n",
    "            for line in lines:\n",
    "                if abs(cy - line[\"cy\"]) < 15:       # 15-px threshold\n",
    "                    line[\"words\"].append(w)\n",
    "                    line[\"cy\"] = (line[\"cy\"] + cy) / 2\n",
    "                    placed = True\n",
    "                    break\n",
    "            if not placed:\n",
    "                lines.append({\"cy\": cy, \"words\": [w]})\n",
    "\n",
    "        # â”€â”€ crop each line robustly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        page = Image.open(page_path).convert(\"L\")\n",
    "        W_page, H_page = page.size\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            bxs, bys, bxe, bye = [], [], [], []\n",
    "            for w in line[\"words\"]:\n",
    "                x  = w[\"x\"]\n",
    "                y  = w[\"y\"]\n",
    "                w_ = w.get(\"w\", w.get(\"width\",  0))\n",
    "                h_ = w.get(\"h\", w.get(\"height\", 0))\n",
    "                # some JSONs store x1/y1 directly\n",
    "                x1 = w.get(\"x1\", x + w_)\n",
    "                y1 = w.get(\"y1\", y + h_)\n",
    "\n",
    "                bxs.append(x);      bys.append(y)\n",
    "                bxe.append(x1);     bye.append(y1)\n",
    "\n",
    "            # fallback: if every y1 == y (heights missing)\n",
    "            if max(bye) == min(bys):\n",
    "                est_line_h = int(H_page * 0.05)     # â‰ˆ 5 % page height\n",
    "                bye = [y + est_line_h for y in bys]\n",
    "\n",
    "            x0 = max(min(bxs) - pad_horz, 0)\n",
    "            y0 = max(min(bys) - pad_vert, 0)\n",
    "            x1 = min(max(bxe) + pad_horz, W_page)\n",
    "            y1 = min(max(bye) + pad_vert, H_page)\n",
    "\n",
    "            crop = page.crop((x0, y0, x1, y1))\n",
    "            crop_name = f\"image_{uid}_line{idx:02d}.png\"\n",
    "            crop.save(CROP_DIR / crop_name)\n",
    "\n",
    "            sorted_words = sorted(line[\"words\"], key=lambda w: w[\"x\"])\n",
    "            text = \" \".join(w[\"text\"] for w in sorted_words).strip()\n",
    "            label_map[crop_name] = text\n",
    "            line_counts[uid] += 1\n",
    "                \n",
    "        for ann in words:\n",
    "            txt = ann.get(\"text\", \"\")\n",
    "            all_clusters.update(get_clusters(txt))\n",
    "            \n",
    "charset = sorted(all_clusters)\n",
    "\n",
    "# Ensure the space character is included in the charset\n",
    "if ' ' not in charset:\n",
    "    charset.append(' ')\n",
    "\n",
    "stoi     = {c: i + 1 for i, c in enumerate(charset)}   # 0 = blank\n",
    "itos     = {i: c for c, i in stoi.items()}\n",
    "\n",
    "print(\"Unique stoi clusters:\", stoi)\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Unique grapheme clusters:\", charset)\n",
    "print(\"Total unique clusters:\", len(charset))\n",
    "print(f\"âœ… Saved {len(label_map)} line crops.\")\n",
    "print(\"Avg lines per page:\", sum(line_counts.values()) / len(line_counts))\n",
    "\n",
    "vocab_path = CROP_DIR / \"chars.txt\"\n",
    "with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for cluster in charset:\n",
    "        f.write(cluster + \"\\n\")\n",
    "\n",
    "# ---------- dump to disk ----------\n",
    "out_path = CROP_DIR / \"labels.json\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"labels.json written with {len(label_map)} entries â†’ {out_path}\")\n",
    "\n",
    "def resize_and_pad(img: Image.Image,\n",
    "                   target_size=(32, 512),\n",
    "                   fill=255) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Keeps aspect ratio, resizes by height, then right-pads (or bottom-pads)\n",
    "    with the `fill` colour (white = 255 for â€œLâ€ mode).\n",
    "    \"\"\"\n",
    "    tgt_h, tgt_w = target_size\n",
    "    w, h = img.size\n",
    "    scale = tgt_h / h\n",
    "    new_w = int(w * scale)\n",
    "    img_rs = img.resize((new_w, tgt_h), Image.BILINEAR)\n",
    "\n",
    "    # if the resized width exceeds target â†’ centre-crop, else pad\n",
    "    if new_w >= tgt_w:\n",
    "        img_rs = img_rs.crop((0, 0, tgt_w, tgt_h))\n",
    "        return img_rs\n",
    "    else:\n",
    "        pad_w = tgt_w - new_w\n",
    "        padding = (0, 0, pad_w, 0)              # (left, top, right, bottom)\n",
    "        return ImageOps.expand(img_rs, padding, fill=fill)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3.  DATASET & LOADER  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class LineOCRDataset(Dataset):\n",
    "    def __init__(self, crop_dir, label_map, stoi, transform):\n",
    "        self.paths = sorted([crop_dir/p for p in os.listdir(crop_dir)\n",
    "                             if p.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
    "        self.label_map, self.stoi, self.transform = label_map, stoi, transform\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(f\"Processing image {idx+1}/{len(self)}: {self.paths[idx].name}\")  \n",
    "        img  = Image.open(self.paths[idx]).convert(\"L\")\n",
    "           # Show the original image before any transform\n",
    "        # for i in range(idx <5): \n",
    "        #     plt.imshow(img, cmap=\"gray\")\n",
    "        #     # plt.title(f\"Original Image: '{self.label_map[self.paths[idx].name]}'\")\n",
    "        #     plt.axis(\"off\")\n",
    "        #     plt.show()\n",
    "   \n",
    "        if self.transform: \n",
    "            img  = resize_and_pad(img, target_size=(IMG_H, IMG_W))\n",
    "            \n",
    "            img = self.transform(img)\n",
    "        text = self.label_map[self.paths[idx].name]\n",
    "        clusters = regex.findall(r\"\\X\", text)\n",
    "        label = torch.tensor([self.stoi[c] for c in clusters if c in self.stoi],\n",
    "                             dtype=torch.long)\n",
    "                # img_t is a tensor of shape (1, IMG_H, IMG_W)\n",
    "        # # Undo normalization if needed (your normalization is mean=0.5, std=0.5)\n",
    "        # img_vis = img * 0.5 + 0.5  # unnormalize to [0,1]\n",
    "\n",
    "        # # Convert to PIL image for display\n",
    "        # img_pil = F.to_pil_image(img_vis)\n",
    "\n",
    "        # for i in range(idx <5):  # or any condition\n",
    "        #     plt.imshow(img_pil, cmap=\"gray\")\n",
    "        #     # plt.title(f\"Transformed Image: '{text}'\")\n",
    "        #     plt.axis(\"off\")\n",
    "        #     plt.show()\n",
    "\n",
    "        return img, label, text           # keep raw text for metrics\n",
    "\n",
    "def collate(batch):\n",
    "    imgs, labels, gts = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    tgt_lens = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    labels = torch.cat(labels)\n",
    "    return imgs, labels, tgt_lens, list(gts)\n",
    "\n",
    "transforms = T.Compose([\n",
    "    T.Resize((IMG_H, IMG_W)),           # fixed width â‡’ fixed T\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# transforms identical â€¦\n",
    "dataset = LineOCRDataset(CROP_DIR, label_map, stoi, transforms)\n",
    "\n",
    "# reproducible 80 / 20 split\n",
    "g = torch.Generator().manual_seed(42)\n",
    "n_val   = max(1, int(0.2 * len(dataset)))\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [len(dataset)-n_val, n_val], generator=g)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate, num_workers=0)\n",
    "\n",
    "# Check the first batch from the train_loader\n",
    "for batch_idx, (imgs, labels, tgt_lens, gts) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  Images shape: {imgs.shape}\")      # (batch_size, 1, IMG_H, IMG_W)\n",
    "    print(f\"  Labels shape: {labels.shape}\")    # (sum of label lengths,)\n",
    "    print(f\"  Target lengths: {tgt_lens.tolist()}\")  # List of label lengths\n",
    "    print(f\"  Ground truth texts: {gts}\")       # List of raw text strings\n",
    "\n",
    "    # Optionally, decode the first label in the batch\n",
    "    start = 0\n",
    "    for i, l in enumerate(tgt_lens):\n",
    "        label_indices = labels[start:start+l]\n",
    "        print(f\"  Sample {i} label indices: {label_indices.tolist()}\")\n",
    "        start += l\n",
    "    break  # Only check the first batch\n",
    "\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate, num_workers=0)\n",
    "\n",
    "print(f\"Dataset size {len(dataset)} â†’ train {len(train_ds)}, val {len(val_ds)}\")\n",
    "\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4.  MODEL  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, n_chars):\n",
    "        super().__init__()\n",
    "        base = models.resnet18()\n",
    "\n",
    "        # keep full width resolution\n",
    "        base.conv1.stride = (1, 1)\n",
    "        base.maxpool = nn.Identity()\n",
    "        for layer in [base.layer2, base.layer3, base.layer4]:\n",
    "            for m in layer:\n",
    "                if hasattr(m, \"conv1\") and m.conv1.stride == (2, 2):\n",
    "                    m.conv1.stride = (2, 1)\n",
    "                    if m.downsample:\n",
    "                        m.downsample[0].stride = (2, 1)\n",
    "\n",
    "        self.encoder = nn.Sequential(*list(base.children())[:-2])  # [B,512,H',W']\n",
    "        feat_h = IMG_H // 8                                        # height â†“ 4\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=512 * feat_h,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(512, n_chars + 1)  # + blank\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(1, 3, 1, 1)            # gray â†’ 3-chan5\n",
    "        feat = self.encoder(x)              # [B,512,H',W']\n",
    "        B, C, H, W = feat.size()\n",
    "        seq = feat.permute(0, 3, 1, 2).contiguous().view(B, W, C * H)\n",
    "        rnn_out, _ = self.rnn(seq)          # [B,W,512]\n",
    "        logits = self.classifier(rnn_out)   # [B,W,n+1]\n",
    "        return logits.log_softmax(-1).permute(1, 0, 2)  # [T,B,C]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5.  TRAINING  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dataset = LineOCRDataset(CROP_DIR, label_map, stoi, transforms)\n",
    "loader  = DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                     shuffle=True, collate_fn=collate, num_workers=0)\n",
    "\n",
    "model     = CRNN(len(stoi)).to(DEVICE)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "FREEZE_EPOCHS = 3               # freeze ResNet for the first N epochs\n",
    "for p in model.encoder.parameters(): p.requires_grad = False\n",
    "\n",
    "blank_idx = 0\n",
    "itos_full = {0:\"\"} | itos       # map 0â†’\"\" for easy decoding\n",
    "\n",
    "def greedy_decode(log_probs):                     # [T,B,C]\n",
    "    best = log_probs.argmax(-1).cpu().T           # [B,T]\n",
    "    sentences = []\n",
    "    for seq in best:\n",
    "        dedup = [k for k,_ in itertools.groupby(seq.tolist()) if k != blank_idx]\n",
    "        sentences.append(\"\".join(itos_full[i] for i in dedup))\n",
    "    return sentences\n",
    "\n",
    "def cer(ref, hyp):\n",
    "    import numpy as np\n",
    "    m, n = len(ref), len(hyp)\n",
    "    dp = np.zeros((m+1, n+1), int)\n",
    "    dp[:,0] = range(m+1); dp[0] = range(n+1)\n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            dp[i,j] = min(dp[i-1,j]+1, dp[i,j-1]+1,\n",
    "                          dp[i-1,j-1] + (ref[i-1]!=hyp[j-1]))\n",
    "    return dp[m,n] / max(1,m)\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "    total_loss, chars, cer_sum, word_hit, total_words = 0, 0, 0, 0, 0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for imgs, labels, tgt_lens, gts in loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            logits = model(imgs)                # [T,B,C]\n",
    "            Tt, B, _ = logits.shape\n",
    "            inp_lens = torch.full((B,), Tt, dtype=torch.long, device=DEVICE)\n",
    "            loss = criterion(logits, labels, inp_lens, tgt_lens)\n",
    "            if train:\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            total_loss += loss.item() * B\n",
    "\n",
    "            preds = greedy_decode(logits)\n",
    "            for gt, pr in zip(gts, preds):\n",
    "                cer_sum    += cer(gt, pr) * len(gt)\n",
    "                chars      += len(gt)\n",
    "                word_hit   += (gt == pr)\n",
    "                total_words += 1\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    cer_rate = cer_sum / chars\n",
    "    word_acc = 100 * word_hit / total_words\n",
    "    return avg_loss, cer_rate, word_acc\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6.  TRAIN LOOP  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    if ep == FREEZE_EPOCHS+1:                 # unfreeze backbone\n",
    "        for p in model.encoder.parameters(): p.requires_grad = True\n",
    "        print(\"ğŸŸ¢ ResNet encoder unfrozen\")\n",
    "\n",
    "    tr_loss, tr_cer, tr_wa = run_epoch(train_loader, train=True)\n",
    "    vl_loss, vl_cer, vl_wa = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(f\"Epoch {ep:02d}/{EPOCHS} âŸ¶ \"\n",
    "          f\"train loss {tr_loss:.3f} | CER {tr_cer:.2%} | W-Acc {tr_wa:.1f}%   ||   \"\n",
    "          f\"val loss {vl_loss:.3f} | CER {vl_cer:.2%} | W-Acc {vl_wa:.1f}%\")\n",
    "\n",
    "    if ep % 5 == 0:\n",
    "        torch.save(model.state_dict(), CKPT_DIR / f\"crnn_ep{ep}.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), CKPT_DIR / \"crnn_final.pth\")\n",
    "print(\"ğŸ‰ Done â€“ model + metrics saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18112\\4088571608.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(CKPT_PATH, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded checkpoint: D:\\python\\data\\line_crops\\checkpoints\\crnn_final.pth\n",
      "Predicted indices: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "ğŸ–¼  Image : image_0a35ad3f-c03b-4dee-8ac6-3e18b129a587_line07.png\n",
      "GT : à¶´à·™à¶¶à¶»à·€à·à¶»à·’ 10 à·€à·à¶±à·’ à¶¯à· à¶½à·’à¶´à·’à¶ºà·™à¶±à·Š à¶‘à¶ºà¶§ à¶‰à¶©à¶šà·Š à¶±à·œ\n",
      "PRD: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â¬›â¬›â¬›  QUICK INFERENCE CELL (no training code)  â¬›â¬›â¬›\n",
    "import json, random, regex, torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# â”€â”€â”€ 1.  PATHS (edit as needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CROP_DIR  = Path(r\"D:\\python\\data\\line_crops\")      # contains crops + chars.txt + labels.json\n",
    "CKPT_PATH = CROP_DIR / \"checkpoints\" / \"crnn_final.pth\"\n",
    "IMG_H, IMG_W = 32, 512                              # must match training\n",
    "\n",
    "# â”€â”€â”€ 2.  CHARSET â†’ stoi / itos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(CROP_DIR / \"chars.txt\", encoding=\"utf-8\") as f:\n",
    "    chars = [ln.rstrip() for ln in f if ln.strip()]\n",
    "if \" \" not in chars:                                # safety check\n",
    "    chars.append(\" \")\n",
    "stoi = {c: i + 1 for i, c in enumerate(chars)}      # 0 = blank\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "# â”€â”€â”€ 3.  IMAGE TRANSFORM (same as training) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "transform = T.Compose([\n",
    "    T.Resize((IMG_H, IMG_W)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# â”€â”€â”€ 4.  CRNN ARCH (identical to training) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, n_chars):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        base.conv1.stride = (1, 1)\n",
    "        base.maxpool = nn.Identity()\n",
    "        for layer in [base.layer2, base.layer3, base.layer4]:\n",
    "            for m in layer:\n",
    "                if hasattr(m, \"conv1\") and m.conv1.stride == (2, 2):\n",
    "                    m.conv1.stride = (2, 1)\n",
    "                    if m.downsample:\n",
    "                        m.downsample[0].stride = (2, 1)\n",
    "        self.encoder = nn.Sequential(*list(base.children())[:-2])  # [B,512,H',W']\n",
    "        feat_h = IMG_H // 8\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=512 * feat_h,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(512, n_chars + 1)  # + blank\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(1, 3, 1, 1)              # gray â†’ 3-chan\n",
    "        feat = self.encoder(x)                # [B,512,H',W']\n",
    "        B, C, H, W = feat.shape\n",
    "        seq = feat.permute(0, 3, 1, 2).contiguous().view(B, W, C*H)\n",
    "        rnn_out, _ = self.rnn(seq)            # [B,W,512]\n",
    "        logits = self.classifier(rnn_out).log_softmax(-1)  # [B,W,n+1]\n",
    "        return logits.permute(1, 0, 2)        # [T,B,C]\n",
    "\n",
    "# â”€â”€â”€ 5.  LOAD MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CRNN(len(stoi)).to(DEVICE)\n",
    "model.load_state_dict(torch.load(CKPT_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "print(\"âœ… Loaded checkpoint:\", CKPT_PATH)\n",
    "\n",
    "# â”€â”€â”€ 6.  PICK ONE RANDOM CROP & GROUND TRUTH â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(CROP_DIR / \"labels.json\", encoding=\"utf-8\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "img_name = random.choice(list(label_map.keys()))\n",
    "gt_text  = label_map[img_name]\n",
    "\n",
    "img = Image.open(CROP_DIR / img_name).convert(\"L\")\n",
    "inp = transform(img).unsqueeze(0).to(DEVICE)        # [1,1,H,W]\n",
    "\n",
    "# â”€â”€â”€ 7.  INFERENCE + GREEDY DECODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with torch.no_grad():\n",
    "    logits = model(inp)                 # [T,1,C]\n",
    "    pred = logits.argmax(-1).squeeze(1).cpu().numpy()  # [T]\n",
    "print(\"Predicted indices:\", pred)\n",
    "\n",
    "decoded = []\n",
    "prev = -1\n",
    "for p in pred:                          # collapse repeats & drop blanks\n",
    "    if p != prev and p != 0:\n",
    "        decoded.append(itos[p])\n",
    "    prev = p\n",
    "pred_text = \"\".join(decoded)\n",
    "\n",
    "# â”€â”€â”€ 8.  DISPLAY RESULT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ–¼  Image :\", img_name)\n",
    "print(\"GT :\", gt_text)\n",
    "print(\"PRD:\", pred_text)\n",
    "\n",
    "plt.imshow(np.array(img), cmap=\"gray\")\n",
    "plt.title(\"Ground-truth vs. Prediction\", fontsize=8)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
